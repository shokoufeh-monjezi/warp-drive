{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b9dd87",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc. \\\n",
    "All rights reserved. \\\n",
    "SPDX-License-Identifier: BSD-3-Clause \\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd7473",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6dab6",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43a361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089b781",
   "metadata": {},
   "source": [
    "# Welcome to WarpDrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b2396",
   "metadata": {},
   "source": [
    "This is the second tutorial on WarpDrive, a PyCUDA-based framework for extremely parallelized multi-agent reinforcement learning (RL) on a single graphics processing unit (GPU). At this stage, we assume you have read our [first tutorial](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb) on WarpDrive basics.\n",
    "\n",
    "In this tutorial, we describe **CUDASampler**, a lightweight and fast action sampler based on the policy distribution across several RL agents and environment replicas. `CUDASampler` utilizes the GPU to parallelize operations to efficiently sample a large number of actions in parallel. \n",
    "\n",
    "Notably:\n",
    "\n",
    "1. It reads the distribution on the GPU through Pytorch and samples actions exclusively at the GPU. There is no data transfer. \n",
    "2. It maximizes parallelism down to the individual thread level, i.e., each agent at each environment has its own random seed and independent random sampling process. \n",
    "3. It runs much faster than most GPU samplers. For example, it is significantly faster than Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50ee24",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e1183",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "On Colab, we will do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e71110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rl_warp_drive in /home/jupyter/.local/lib/python3.8/site-packages (1.6.7)\n",
      "Requirement already satisfied: pycuda==2021.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (2021.1)\n",
      "Requirement already satisfied: gym>=0.18 in /home/jupyter/.local/lib/python3.8/site-packages (from rl_warp_drive) (0.25.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (5.4.1)\n",
      "Requirement already satisfied: pytest>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (6.2.4)\n",
      "Requirement already satisfied: torch<1.11,>=1.9 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (1.10.0a0+3fd9dcf)\n",
      "Requirement already satisfied: matplotlib>=3.2.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (1.21.2)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (1.4.4)\n",
      "Requirement already satisfied: mako in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (1.2.0)\n",
      "Requirement already satisfied: pytools>=2011.2 in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (2022.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (2.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (0.0.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyter/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.18->rl_warp_drive) (3.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.2.1->rl_warp_drive) (1.16.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (0.13.1)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (1.1.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (1.10.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (0.10.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (21.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (21.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytools>=2011.2->pycuda==2021.1->rl_warp_drive) (2.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/lib/python3.8/site-packages (from pytools>=2011.2->pycuda==2021.1->rl_warp_drive) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.8/site-packages (from mako->pycuda==2021.1->rl_warp_drive) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! git clone https://github.com/salesforce/warp-drive.git\n",
    "    % cd warp-drive\n",
    "    ! pip install -e .\n",
    "else:\n",
    "    ! pip install -U rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9768a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from warp_drive.managers.function_manager import CUDAFunctionManager, CUDASampler\n",
    "from warp_drive.managers.data_manager import CUDADataManager\n",
    "from warp_drive.utils.constants import Constants\n",
    "from warp_drive.utils.data_feed import DataFeed\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "_MAIN_FILEPATH = f\"{get_project_root()}/warp_drive/cuda_includes\"\n",
    "_CUBIN_FILEPATH = f\"{get_project_root()}/warp_drive/cuda_bin\"\n",
    "_ACTIONS = Constants.ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663a5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38e757",
   "metadata": {},
   "source": [
    "# Initialize CUDASampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a2443",
   "metadata": {},
   "source": [
    "We first initialize the **CUDADataManager** and **CUDAFunctionManager**. To illustrate the sampler, we first load a pre-compiled binary file called \"test_build.cubin\". Note that these low-level managers and modules will be hidden and called automatically by WarpDrive in any end-to-end training and simulation. In this and the next tutorials, we want to show how a few fundamental modules work and their performance, that is why some low-level APIs are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed013b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(11,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(2,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(2,)\n"
     ]
    }
   ],
   "source": [
    "cuda_data_manager = CUDADataManager(num_agents=5, episode_length=10, num_envs=2)\n",
    "cuda_function_manager = CUDAFunctionManager(\n",
    "    num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "    num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    ")\n",
    "\n",
    "main_example_file = f\"{_MAIN_FILEPATH}/test_build.cu\"\n",
    "bin_example_file = f\"{_CUBIN_FILEPATH}/test_build.fatbin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c882ac",
   "metadata": {},
   "source": [
    "This binary is compiled with inclusion of auxiliary files in `warp_drive/cuda_includes/core` which includes several CUDA core services provided by WarpDrive. These include the backend source code for `CUDASampleController`. \n",
    "\n",
    "To make \"test_build.fatbin\" available, we compiled this test cubin by calling `_compile()` from `CUDAFunctionManager`.\n",
    "For this notebook demonstration, in the bin folder, we have already provided a pre-compiled binary but we suggest that you still execute the cell below to re-compile it to avoid possilble binary incompatible issues across different platforms. (`_compile()` is a low-level API, user will not need call those internal APIs directly for any WarpDrive end-to-end simulation and training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b5bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully mkdir the binary folder /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_includes/../../example_envs/tag_gridworld/tag_gridworld_step.cu(151): warning: invalid narrowing conversion from \"unsigned int\" to \"int\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cmd: nvcc --fatbin -arch=sm_70 /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_includes/test_build.cu -o /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_bin/test_build.fatbin\n",
      "INFO:root:Successfully build the cubin_file from /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_includes/test_build.cu to /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_bin/test_build.fatbin\n"
     ]
    }
   ],
   "source": [
    "cuda_function_manager._compile(main_file=main_example_file, \n",
    "                               cubin_file=bin_example_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00fd3d",
   "metadata": {},
   "source": [
    "Finally, we initialize **CUDASampler** and assign the random seed. `CUDASampler` keeps independent randomness across all threads and blocks. Notice that `CUDASampler` requires `CUDAFunctionManager` because `CUDAFunctionManager` manages all the CUDA function pointers including to the sampler. Also notice this test binary uses 2 environment replicas and 5 agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41e0f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully load the cubin_file from /home/jupyter/.local/lib/python3.8/site-packages/warp_drive/cuda_bin/test_build.fatbin\n",
      "INFO:root:starting to load the cuda kernel function: reset_log_mask from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_log_mask from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: update_log_mask from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: update_log_mask from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: log_one_step_in_float from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: log_one_step_in_float from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: log_one_step_in_int from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: log_one_step_in_int from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_float_when_done_2d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_float_when_done_2d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_int_when_done_2d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_int_when_done_2d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_float_when_done_3d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_float_when_done_3d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_int_when_done_3d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_int_when_done_3d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: undo_done_flag_and_reset_timestep from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: undo_done_flag_and_reset_timestep from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: init_random from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: init_random from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: free_random from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: free_random from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: sample_actions from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: sample_actions from the CUDA module, \n",
      "INFO:root:Successfully initialize the default CUDA functions managed by the CUDAFunctionManager\n",
      "INFO:root:random seed is not provided, by default, using the current timestamp 1658964090.636753 as seed\n"
     ]
    }
   ],
   "source": [
    "cuda_function_manager.load_cuda_from_binary_file(\n",
    "    bin_example_file, default_functions_included=True\n",
    ")\n",
    "cuda_sampler = CUDASampler(function_manager=cuda_function_manager)\n",
    "cuda_sampler.init_random(seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2b34d",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a9939",
   "metadata": {},
   "source": [
    "## Actions Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733ce7a",
   "metadata": {},
   "source": [
    "Now, we feed the **actions_a** placeholder into the GPU. It has the shape `(n_envs=2, n_agents=5)` as expected. Also we make it accessible by Pytorch, because during RL training, actions will be fed into the Pytorch trainer directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'sampled_actions_a' from type int64 to int32\n",
      "INFO:root:- sampled_actions_a                                                               : dtype=int32     , shape=(2, 5)\n"
     ]
    }
   ],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(name=f\"{_ACTIONS}_a\", data=[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=True)\n",
    "assert cuda_data_manager.is_data_on_device_via_torch(f\"{_ACTIONS}_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838464d4",
   "metadata": {},
   "source": [
    "## Action Sampled Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ad05c",
   "metadata": {},
   "source": [
    "We define an action **distribution** here. During training, this distribution would be provided by the policy model implemented in Pytorch. The distribution has the shape `(n_envs, n_agents, **n_actions**)`. The last dimension `n_actions` defines the size of the action space for a particular *discrete* action. For example, if we have up, down, left, right and no-ops, `n_actions=5`.\n",
    "\n",
    "**n_actions** needs to be registered by the sampler so the sampler is able to pre-allocate a global memory space in GPU to speed up action sampling. This can be done by calling `sampler.register_actions()`.\n",
    "\n",
    "In this tutorial, we check if our sampled action distribution follows the given distribution. For example, the distribution [0.333, 0.333, 0.333] below suggests the 1st agent has 3 possible actions and each of them have equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abdd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- sampled_actions_a_cum_distr                                                     : dtype=float32   , shape=(2, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "cuda_sampler.register_actions(\n",
    "    cuda_data_manager, action_name=f\"{_ACTIONS}_a\", num_actions=3\n",
    ")\n",
    "\n",
    "distribution = np.array(\n",
    "    [\n",
    "        [\n",
    "            [0.333, 0.333, 0.333],\n",
    "            [0.2, 0.5, 0.3],\n",
    "            [0.95, 0.02, 0.03],\n",
    "            [0.02, 0.95, 0.03],\n",
    "            [0.02, 0.03, 0.95],\n",
    "        ],\n",
    "        [\n",
    "            [0.1, 0.7, 0.2],\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.5, 0.5, 0.0],\n",
    "            [0.0, 0.5, 0.5],\n",
    "            [0.5, 0.0, 0.5],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "distribution = torch.from_numpy(distribution).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392d32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10000 times to collect statistics\n",
    "actions_batch = torch.from_numpy(np.empty((10000, 2, 5), dtype=np.int32)).cuda()\n",
    "\n",
    "for i in range(10000):\n",
    "    cuda_sampler.sample(cuda_data_manager, distribution, action_name=f\"{_ACTIONS}_a\")\n",
    "    actions_batch[i] = cuda_data_manager.data_on_device_via_torch(f\"{_ACTIONS}_a\")\n",
    "actions_batch_host = actions_batch.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcb90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_env_0 = actions_batch_host[:, 0]\n",
    "actions_env_1 = actions_batch_host[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9132a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled actions distribution versus the given distribution (in bracket) for env 0: \n",
      "\n",
      "Sampled action distribution for agent_id: 0:\n",
      "0.3368(0.3330000042915344), \n",
      "0.3252(0.3330000042915344), \n",
      "0.338(0.3330000042915344)  \n",
      "\n",
      "Sampled action distribution for agent_id: 1:\n",
      "0.2018(0.20000000298023224), \n",
      "0.4954(0.5), \n",
      "0.3028(0.30000001192092896)  \n",
      "\n",
      "Sampled action distribution for agent_id: 2:\n",
      "0.9528(0.949999988079071), \n",
      "0.0193(0.019999999552965164), \n",
      "0.0279(0.029999999329447746)  \n",
      "\n",
      "Sampled action distribution for agent_id: 3:\n",
      "0.0183(0.019999999552965164), \n",
      "0.9501(0.949999988079071), \n",
      "0.0316(0.029999999329447746)  \n",
      "\n",
      "Sampled action distribution for agent_id: 4:\n",
      "0.0208(0.019999999552965164), \n",
      "0.032(0.029999999329447746), \n",
      "0.9472(0.949999988079071)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sampled actions distribution versus the given distribution (in bracket) for env 0: \\n\"\n",
    ")\n",
    "for agent_id in range(5):\n",
    "    print(\n",
    "        f\"Sampled action distribution for agent_id: {agent_id}:\\n\"\n",
    "        f\"{(actions_env_0[:, agent_id] == 0).sum() / 10000.0}({distribution[0, agent_id, 0]}), \\n\"\n",
    "        f\"{(actions_env_0[:, agent_id] == 1).sum() / 10000.0}({distribution[0, agent_id, 1]}), \\n\"\n",
    "        f\"{(actions_env_0[:, agent_id] == 2).sum() / 10000.0}({distribution[0, agent_id, 2]})  \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9d9ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled actions distribution versus the given distribution (in bracket) for env 1: \n",
      "Sampled action distribution for agent_id: 0:\n",
      "0.0998(0.10000000149011612), \n",
      "0.7032(0.699999988079071), \n",
      "0.197(0.20000000298023224)  \n",
      "\n",
      "Sampled action distribution for agent_id: 1:\n",
      "0.6898(0.699999988079071), \n",
      "0.209(0.20000000298023224), \n",
      "0.1012(0.10000000149011612)  \n",
      "\n",
      "Sampled action distribution for agent_id: 2:\n",
      "0.4943(0.5), \n",
      "0.5057(0.5), \n",
      "0.0(0.0)  \n",
      "\n",
      "Sampled action distribution for agent_id: 3:\n",
      "0.0(0.0), \n",
      "0.4979(0.5), \n",
      "0.5021(0.5)  \n",
      "\n",
      "Sampled action distribution for agent_id: 4:\n",
      "0.4923(0.5), \n",
      "0.0(0.0), \n",
      "0.5077(0.5)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sampled actions distribution versus the given distribution (in bracket) for env 1: \"\n",
    ")\n",
    "\n",
    "for agent_id in range(5):\n",
    "    print(\n",
    "        f\"Sampled action distribution for agent_id: {agent_id}:\\n\"\n",
    "        f\"{(actions_env_1[:, agent_id] == 0).sum() / 10000.0}({distribution[1, agent_id, 0]}), \\n\"\n",
    "        f\"{(actions_env_1[:, agent_id] == 1).sum() / 10000.0}({distribution[1, agent_id, 1]}), \\n\"\n",
    "        f\"{(actions_env_1[:, agent_id] == 2).sum() / 10000.0}({distribution[1, agent_id, 2]})  \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdd6e8",
   "metadata": {},
   "source": [
    "## Action Randomness Across Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e24c1",
   "metadata": {},
   "source": [
    "Another important validation is whether the sampler provides independent randomness across different agents and environment replicas. Given the same policy model for all the agents and environment replicas, we can check if the sampled actions are independently distributed. \n",
    "\n",
    "Here, we assign all agents across all envs the same distribution [0.25, 0.25, 0.25, 0.25]. It is equivalent to an uniform action distribution among all actions [0,1,2,3], across 5 agents and 2 envs. Then we check the standard deviation across the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4436d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'sampled_actions_b' from type int64 to int32\n",
      "INFO:root:- sampled_actions_b                                                               : dtype=int32     , shape=(2, 5)\n"
     ]
    }
   ],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(name=f\"{_ACTIONS}_b\", data=[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=True)\n",
    "assert cuda_data_manager.is_data_on_device_via_torch(f\"{_ACTIONS}_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43c61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- sampled_actions_b_cum_distr                                                     : dtype=float32   , shape=(2, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "cuda_sampler.register_actions(\n",
    "    cuda_data_manager, action_name=f\"{_ACTIONS}_b\", num_actions=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe975fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.array(\n",
    "    [\n",
    "        [\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "        ],\n",
    "        [\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "            [0.25, 0.25, 0.25, 0.25],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "distribution = torch.from_numpy(distribution).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e793f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10000 times to collect statistics.\n",
    "actions_batch = torch.from_numpy(np.empty((10000, 2, 5), dtype=np.int32)).cuda()\n",
    "\n",
    "for i in range(10000):\n",
    "    cuda_sampler.sample(cuda_data_manager, distribution, action_name=f\"{_ACTIONS}_b\")\n",
    "    actions_batch[i] = cuda_data_manager.data_on_device_via_torch(f\"{_ACTIONS}_b\")\n",
    "actions_batch_host = actions_batch.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663e77c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 0, 1, 1],\n",
       "        [1, 0, 2, 3, 1]],\n",
       "\n",
       "       [[1, 3, 0, 3, 1],\n",
       "        [3, 3, 3, 3, 1]],\n",
       "\n",
       "       [[0, 1, 2, 2, 2],\n",
       "        [3, 2, 1, 0, 1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3, 0, 2, 2, 2],\n",
       "        [1, 1, 1, 0, 1]],\n",
       "\n",
       "       [[1, 1, 0, 1, 0],\n",
       "        [1, 0, 2, 3, 0]],\n",
       "\n",
       "       [[2, 2, 0, 1, 0],\n",
       "        [1, 2, 2, 2, 3]]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_batch_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1f5432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96386563, 0.96353072])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_batch_host.std(axis=2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586caa8c",
   "metadata": {},
   "source": [
    "To check the independence of randomness among all threads, we can compare it with a Numpy implementation. Here we use `numpy.choice(4, 5)` to repeat the same process for an uniform action distribution among all actions [0,1,2,3], 5 agents and 2 envs. We should see that the variation of Numpy output is very close to our sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fb754c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96419519, 0.96432735])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_batch_numpy = np.empty((10000, 2, 5), dtype=np.int32)\n",
    "for i in range(10000):\n",
    "    actions_batch_numpy[i, 0, :] = np.random.choice(4, 5)\n",
    "    actions_batch_numpy[i, 1, :] = np.random.choice(4, 5)\n",
    "actions_batch_numpy.std(axis=2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dab428",
   "metadata": {},
   "source": [
    "## Running Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f44d5",
   "metadata": {},
   "source": [
    "The total time for sampling includes receiving a new distribution and using this to sample.\n",
    "Comparing our sampler with [torch.Categorical sampler](https://pytorch.org/docs/stable/distributions.html), \n",
    "we reach **7-8X** speed up for the distribution above. \n",
    "\n",
    "*Note: our sampler runs in parallel across threads, so this speed-up is almost constant when scaling up the number of agents or environment replicas, i.e., increasing the number of used threads.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c37d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db378403",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.array(\n",
    "    [\n",
    "        [\n",
    "            [0.333, 0.333, 0.333],\n",
    "            [0.2, 0.5, 0.3],\n",
    "            [0.95, 0.02, 0.03],\n",
    "            [0.02, 0.95, 0.03],\n",
    "            [0.02, 0.03, 0.95],\n",
    "        ],\n",
    "        [\n",
    "            [0.1, 0.7, 0.2],\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.5, 0.5, 0.0],\n",
    "            [0.0, 0.5, 0.5],\n",
    "            [0.5, 0.0, 0.5],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "distribution = torch.from_numpy(distribution).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a2dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 26.30963134765625 ms\n"
     ]
    }
   ],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "for _ in range(1000):\n",
    "    cuda_sampler.sample(cuda_data_manager, distribution, action_name=f\"{_ACTIONS}_a\")\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"time elapsed: {start_event.elapsed_time(end_event)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14985bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 320.6543273925781 ms\n"
     ]
    }
   ],
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "for _ in range(1000):\n",
    "    Categorical(distribution).sample()\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"time elapsed: {start_event.elapsed_time(end_event)} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cb0a3",
   "metadata": {},
   "source": [
    "# Learn More and Explore our Tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadb1da",
   "metadata": {},
   "source": [
    "Next, we suggest you check out our advanced [tutorial](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb) on WarpDrive's reset and log controller!\n",
    "\n",
    "For your reference, all our tutorials are here:\n",
    "1. [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "3. [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "4. [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "5. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "6. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "7. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.0__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
