{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b4210a",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da976dac",
   "metadata": {},
   "source": [
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fcc8e",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db9034",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98be199",
   "metadata": {},
   "source": [
    "In this tutorial, we describe how to\n",
    "- Use the WarpDrive framework to perform end-to-end training of multi-agent reinforcement learning (RL) agents.\n",
    "- Visualize the behavior using the trained policies.\n",
    "\n",
    "In case you haven't familiarized yourself with WarpDrive, please see the other tutorials we have prepared for you\n",
    "- [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "- [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "- [WarpDrive reset and log controller](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "\n",
    "Please also see our [tutorial](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md) on creating your own RL environment in CUDA C. Once you have your own environment in CUDA C, this tutorial explains how to integrate it with the WarpDrive framework to perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503daf8",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ab709",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe52f1d-710f-4be3-9fc0-a73ada34d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"rl-warp-drive>=1.6.5\" \"torch==1.10.*\" \"torchvision==0.11.*\" \"torchtext==0.11.*\" \"ffmpeg-python\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21b87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0d0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from example_envs.tag_continuous.generate_rollout_animation import (\n",
    "    generate_tag_env_rollout_animation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf352261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from IPython.display import HTML\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b913593",
   "metadata": {},
   "source": [
    "# Training the continuous version of Tag with WarpDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031a938",
   "metadata": {},
   "source": [
    "We will now explain how to train your environments using WarpDrive in just a few steps. For the sake of exposition, we consider the continuous version of Tag.\n",
    "\n",
    "For your reference, there is also an example end-to-end RL training script [here](https://github.com/salesforce/warp-drive/blob/master/warp_drive/training/example_training_script.py) that contains all the steps below. It can be used to set up your own custom training pipeline. Invoke training by using\n",
    "```shell\n",
    "python warp_drive/training/example_training_script.py --env <ENV-NAME>\n",
    "```\n",
    "where `<ENV-NAME>` can be `tag_gridworld` or `tag_continuous` (or any new env that you build)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec74a3",
   "metadata": {},
   "source": [
    "## Step 1: Specify a set of run configurations for your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3abdcd",
   "metadata": {},
   "source": [
    "In order to run the training for these environments, we first need to specify a *run config*, which comprises the set of environment, training, and model parameters.\n",
    "\n",
    "Note: there are also some default configurations in 'warp_drive/training/run_configs/default_configs.yaml', and the run configurations you provide will override them.\n",
    "\n",
    "For this tutorial, we will use the configuration [here](assets/tag_continuous_training/run_config.yaml). Specifically, we'll use $5$ taggers and $100$ runners in a $20 \\times 20$ square grid. The taggers and runners have the same skill level, i.e., the runners can move just as fast as the taggers.\n",
    "\n",
    "The sequence of snapshots below shows a sample realization of the game with randomly chosen agent actions. The $5$ taggers are marked in pink, while the $100$ blue agents are the runners. The snapshots are taken at 1) the beginning of the episode ($t=0$), 2) timestep $250$, and 3) end of the episode ($t=500$). Only $20\\%$ of the runners remain at the end of the episode.\n",
    "\n",
    "<img src=\"assets/tag_continuous_training/t=0.png\" width=\"250\" height=\"250\"/> <img src=\"assets/tag_continuous_training/t=250.png\" width=\"250\" height=\"250\"/> <img src=\"assets/tag_continuous_training/t=500.png\" width=\"250\" height=\"250\"/>\n",
    "\n",
    "We train the agents using $200$ environments or simulations running in parallel. With WarpDrive, each simulation runs on sepate GPU blocks.\n",
    "\n",
    "There are two separate policy networks used for the tagger and runner agents. Each network is a fully-connected model with two layers each of $256$ dimensions. We use the Advantage Actor Critic (A2C) algorithm for training. WarpDrive also currently provides the option to use the Proximal Policy Optimization (PPO) algorithm instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79777d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the run config.\n",
    "\n",
    "# Here we show an example configures\n",
    "\n",
    "CFG = \"\"\"\n",
    "# Sample YAML configuration for the tag continuous environment\n",
    "name: \"tag_continuous\"\n",
    "\n",
    "# Environment settings\n",
    "env:\n",
    "    num_taggers: 5\n",
    "    num_runners: 100\n",
    "    grid_length: 20\n",
    "    episode_length: 500\n",
    "    max_acceleration: 0.1\n",
    "    min_acceleration: -0.1\n",
    "    max_turn: 2.35  # 3*pi/4 radians\n",
    "    min_turn: -2.35  # -3*pi/4 radians\n",
    "    num_acceleration_levels: 10\n",
    "    num_turn_levels: 10\n",
    "    skill_level_runner: 1\n",
    "    skill_level_tagger: 1\n",
    "    seed: 274880\n",
    "    use_full_observation: False\n",
    "    runner_exits_game_after_tagged: True\n",
    "    num_other_agents_observed: 10\n",
    "    tag_reward_for_tagger: 10.0\n",
    "    tag_penalty_for_runner: -10.0\n",
    "    step_penalty_for_tagger: -0.00\n",
    "    step_reward_for_runner: 0.00\n",
    "    edge_hit_penalty: -0.0\n",
    "    end_of_game_reward_for_runner: 1.0\n",
    "    tagging_distance: 0.02\n",
    "\n",
    "# Trainer settings\n",
    "trainer:\n",
    "    num_envs: 400 # number of environment replicas\n",
    "    train_batch_size: 10000 # total batch size used for training per iteration (across all the environments)\n",
    "    num_episodes: 500 # number of episodes to run the training for (can be arbitrarily high)\n",
    "# Policy network settings\n",
    "policy: # list all the policies below\n",
    "    runner:\n",
    "        to_train: True # flag indicating whether the model needs to be trained\n",
    "        algorithm: \"A2C\" # algorithm used to train the policy\n",
    "        gamma: 0.98 # discount rate gamms\n",
    "        lr: 0.005 # learning rate\n",
    "        vf_loss_coeff: 1 # loss coefficient for the value function loss\n",
    "        entropy_coeff:\n",
    "        - [0, 0.5]\n",
    "        - [2000000, 0.05]\n",
    "        model: # policy model settings\n",
    "            type: \"fully_connected\" # model type\n",
    "            fc_dims: [256, 256] # dimension(s) of the fully connected layers as a list\n",
    "            model_ckpt_filepath: \"\" # filepath (used to restore a previously saved model)\n",
    "    tagger:\n",
    "        to_train: True\n",
    "        algorithm: \"A2C\"\n",
    "        gamma: 0.98\n",
    "        lr: 0.002\n",
    "        vf_loss_coeff: 1\n",
    "        model:\n",
    "            type: \"fully_connected\"\n",
    "            fc_dims: [256, 256]\n",
    "            model_ckpt_filepath: \"\"\n",
    "\n",
    "# Checkpoint saving setting\n",
    "saving:\n",
    "    metrics_log_freq: 100 # how often (in iterations) to print the metrics\n",
    "    model_params_save_freq: 5000 # how often (in iterations) to save the model parameters\n",
    "    basedir: \"/tmp\" # base folder used for saving\n",
    "    name: \"tag_continuous\"\n",
    "    tag: \"100runners_5taggers\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run_config = yaml.safe_load(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9c06c",
   "metadata": {},
   "source": [
    "## Step 2: Create the environment object using WarpDrive's envWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2e501",
   "metadata": {},
   "source": [
    "### Important! Ensure that 'use_cuda' is set to True below (in order to run the simulation on the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ada974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env_wrapper = EnvWrapper(\n",
    "    env_obj=TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf73b8a",
   "metadata": {},
   "source": [
    "Creating the env wrapper initializes the CUDA data manager and pushes some reserved data arrays to the GPU. It also initializes the CUDA function manager, and loads some WarpDrive library CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628cf967",
   "metadata": {},
   "source": [
    "## Step 3: Specify a mapping from the policy to agent indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696b489",
   "metadata": {},
   "source": [
    "Next, we will need to map each trainable policy to the agent indices that are using it. As such, we have the tagger and runner policies, and we will map those to the corresponding agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fda3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec12058",
   "metadata": {},
   "source": [
    "Note that if you wish to use just a single policy across all the agents (or if you wish to use many other policies), you will need to update the run configuration as well as the policy_to_agent_id_mapping.\n",
    "\n",
    "For example, for using a shared policy across all agents (say `shared_policy`), for example, you can just use the run configuration as\n",
    "```python\n",
    "    \"policy\": {\n",
    "        \"shared_policy\": {\n",
    "            \"to_train\": True,\n",
    "            ...\n",
    "        },\n",
    "    },\n",
    "```\n",
    "and also set all the agent ids to use this shared policy\n",
    "```python\n",
    "    policy_tag_to_agent_id_map = {\n",
    "        \"shared_policy\": np.arange(envObj.env.num_agents),\n",
    "    }\n",
    "```\n",
    "\n",
    "**Importantly, make sure the `policy` keys and the `policy_tag_to_agent_id_map` keys are identical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09d445",
   "metadata": {},
   "source": [
    "## Step 4: Create the Trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f55261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(env_wrapper, run_config, policy_tag_to_agent_id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f157ec",
   "metadata": {},
   "source": [
    "The `Trainer` class also takes in a few optional arguments that will need to be correctly set, if required.\n",
    "- `create_separate_placeholders_for_each_policy`: a flag indicating whether there exist separate observations, actions and rewards placeholders, for each policy, used in the step() function and during training. When there's only a single policy, this flag will be False. It can also be True when there are multiple policies, yet all the agents have the same obs and action space shapes, so we can share the same placeholder. Defaults to \"False\".\n",
    "- `obs_dim_corresponding_to_num_agents`: indicative of which dimension in the observation corresponds to the number of agents, as designed in the step function. It may be \"first\" or \"last\". In other words, observations may be shaped (num_agents, feature_dims) or (feature_dims, num_agents). This is required in order for WarpDrive to process the observations correctly. This is only relevant when a single obs key corresponds to multiple agents. Defaults to \"first\".\n",
    "- `num_devices`: number of GPU devices used for (distributed) training. Defaults to 1.\n",
    "- `device_id`: the device ID. This is set in the context of multi-GPU training.\n",
    "- `results_dir`: (optional) name of the directory to save results into. If this is not set, the current timestamp will be used instead.\n",
    "- `verbose`: if False, training metrics are not printed to the screen. Defaults to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c488842",
   "metadata": {},
   "source": [
    "When the trainer object is created, the environment(s) are reset and all the relevant data arrays (e.g., \"loc_x\", \"loc_y, \"speed\") are automatically pushed from the CPU to the GPU (just once). Additionally, the observation, reward, action and done flag data array sizes are determined and the array placeholders are also automatically pushed to the GPU. After training begins, these arrays are only updated in-place, and there's no data transferred back to the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b651c25",
   "metadata": {},
   "source": [
    "# Visualizing the trainer policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22a74d",
   "metadata": {},
   "source": [
    "## Visualizing an episode roll-out before training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791e249",
   "metadata": {},
   "source": [
    "We have created a helper function (`generate_tag_env_rollout_animation`) in order to visualize an episode rollout. Internally, this function uses the WarpDrive module's `fetch_episode_states` to fetch the data arrays on the GPU for the duration of an entire episode. Specifically, we fetch the state arrays pertaining to agents' x and y locations on the plane and indicators on which agents are still active in the game, and will use these to visualize an episode roll-out. Note that this function may be invoked at any time during training, and it will use the state of the policy models at that time to sample actions and generate the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aef4d6d-9ece-442e-8644-a1857fd0aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ffmpeg-python in /home/jupyter/.local/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from ffmpeg-python) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce78b8f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/790665501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the entire episode roll-out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_tag_env_rollout_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1350\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Requested MovieWriter ({name}) not available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3e614",
   "metadata": {},
   "source": [
    "In the visualization above, the large purple dots represent the taggers, while the smaller blue dots represent the runners. Before training, the runners and taggers move around randomly, and that results in a lot of the runners getting tagged, just by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29168f95",
   "metadata": {},
   "source": [
    "## Step 5: Perform training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797680f",
   "metadata": {},
   "source": [
    "Training is performed by calling trainer.train(). We run training for just $500$ episodes, as specified in the run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6afbbad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :      87.12\n",
      "Mean action sample time per iter (ms)   :       5.64\n",
      "Mean env. step time per iter (ms)       :     583.55\n",
      "Mean training time per iter (ms)        :     922.47\n",
      "Mean total time per iter (ms)           :    1605.53\n",
      "Mean steps per sec (policy eval)        :  114778.01\n",
      "Mean steps per sec (action sample)      : 1772345.74\n",
      "Mean steps per sec (env. step)          :   17136.58\n",
      "Mean steps per sec (training time)      :   10840.45\n",
      "Mean steps per sec (total)              :    6228.48\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.50000\n",
      "Total loss                              :   -0.86481\n",
      "Policy loss                             :   -1.75696\n",
      "Value function loss                     :    3.28871\n",
      "Mean rewards                            :   -0.03558\n",
      "Max. rewards                            :    0.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -0.06303\n",
      "Mean advantages                         :   -0.36660\n",
      "Mean (norm.) advantages                 :   -0.36660\n",
      "Mean (discounted) returns               :   -0.42963\n",
      "Mean normalized returns                 :   -0.42963\n",
      "Mean entropy                            :    4.79313\n",
      "Variance explained by the value function:   -0.00001\n",
      "Std. of action_0 over agents            :    3.15108\n",
      "Std. of action_0 over envs              :    3.15347\n",
      "Std. of action_0 over time              :    3.13886\n",
      "Std. of action_1 over agents            :    3.16132\n",
      "Std. of action_1 over envs              :    3.16350\n",
      "Std. of action_1 over time              :    3.15110\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.00000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :  167.42236\n",
      "Policy loss                             :   35.82244\n",
      "Value function loss                     :  131.83960\n",
      "Mean rewards                            :    0.69840\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.03273\n",
      "Mean advantages                         :    7.47403\n",
      "Mean (norm.) advantages                 :    7.47403\n",
      "Mean (discounted) returns               :    7.50676\n",
      "Mean normalized returns                 :    7.50676\n",
      "Mean entropy                            :    4.79338\n",
      "Variance explained by the value function:    0.00012\n",
      "Std. of action_0 over agents            :    3.04757\n",
      "Std. of action_0 over envs              :    3.17742\n",
      "Std. of action_0 over time              :    3.16332\n",
      "Std. of action_1 over agents            :    3.05663\n",
      "Std. of action_1 over envs              :    3.16885\n",
      "Std. of action_1 over time              :    3.15608\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    0.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/tag_continuous/100runners_5taggers/1659043711/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/tag_continuous/100runners_5taggers/1659043711/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/tag_continuous/100runners_5taggers/1659043711/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 25 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :      80.94\n",
      "Mean action sample time per iter (ms)   :       5.16\n",
      "Mean env. step time per iter (ms)       :     115.35\n",
      "Mean training time per iter (ms)        :     130.74\n",
      "Mean total time per iter (ms)           :     335.11\n",
      "Mean steps per sec (policy eval)        :  123545.12\n",
      "Mean steps per sec (action sample)      : 1937255.01\n",
      "Mean steps per sec (env. step)          :   86690.45\n",
      "Mean steps per sec (training time)      :   76487.21\n",
      "Mean steps per sec (total)              :   29840.59\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.44600\n",
      "Total loss                              :   -0.35987\n",
      "Policy loss                             :    0.26634\n",
      "Value function loss                     :    1.51195\n",
      "Mean rewards                            :   -0.02703\n",
      "Max. rewards                            :    0.00000\n",
      "Min. rewards                            :  -10.00000\n",
      "Mean value function                     :   -1.34329\n",
      "Mean advantages                         :    0.05559\n",
      "Mean (norm.) advantages                 :    0.05559\n",
      "Mean (discounted) returns               :   -1.28770\n",
      "Mean normalized returns                 :   -1.28770\n",
      "Mean entropy                            :    4.79409\n",
      "Variance explained by the value function:    0.47839\n",
      "Std. of action_0 over agents            :    3.17685\n",
      "Std. of action_0 over envs              :    3.17935\n",
      "Std. of action_0 over time              :    3.16652\n",
      "Std. of action_1 over agents            :    3.14698\n",
      "Std. of action_1 over envs              :    3.14939\n",
      "Std. of action_1 over time              :    3.13668\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.30435\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    : -866.60250\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   61.31593\n",
      "Policy loss                             :    6.99169\n",
      "Value function loss                     :   54.48438\n",
      "Mean rewards                            :    0.53180\n",
      "Max. rewards                            :   20.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   15.55641\n",
      "Mean advantages                         :    2.17790\n",
      "Mean (norm.) advantages                 :    2.17790\n",
      "Mean (discounted) returns               :   17.73431\n",
      "Mean normalized returns                 :   17.73431\n",
      "Mean entropy                            :    3.20277\n",
      "Variance explained by the value function:   -0.00070\n",
      "Std. of action_0 over agents            :    1.00486\n",
      "Std. of action_0 over envs              :    1.14984\n",
      "Std. of action_0 over time              :    1.12005\n",
      "Std. of action_1 over agents            :    2.51874\n",
      "Std. of action_1 over envs              :    2.64791\n",
      "Std. of action_1 over time              :    2.63273\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.78887\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :  868.05000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/tag_continuous/100runners_5taggers/1659043711/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/tag_continuous/100runners_5taggers/1659043711/runner_250000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/tag_continuous/100runners_5taggers/1659043711/tagger_250000.state_dict'. \n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a4cd6",
   "metadata": {},
   "source": [
    "As training happens, we log the speed performance numbers and the metrics for all the trained policies every `metrics_log_freq` iterations. The training results and the model checkpoints are also saved on a timely (as specified in the run configuration parameters `model_params_save_freq`) basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574dca00",
   "metadata": {},
   "source": [
    "## Visualize an episode-rollout after training (for about 2M episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057889ba",
   "metadata": {},
   "source": [
    "We can also initialize the trainer model parameters using saved model checkpoints via the `load_model_checkpoint` API. With this, we will be able to fetch the episode states for a trained model, for example. We will now visualize an episode roll-out using trained tagger and runner policy model weights (trained for $2$M episodes or $1$B steps), that are located in [this](assets/tag_continuous_training/) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10abdba7-8b93-4cab-8fa3-293d95aaf7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/warp-drive/tutorials\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6dc1f5-3cc0-4d41-80a2-c4956204b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to fix the path to the tutorial in the next cell based on this example\n",
    "\"\"\"\n",
    "trainer.load_model_checkpoint(\n",
    "    {\n",
    "        \"tagger\": \"FIX_ME/tutorials/assets/tag_continuous_training/tagger_1000010000.state_dict\",\n",
    "        \"runner\": \"FIXME/tutorials/assets/tag_continuous_training/runner_1000010000.state_dict\",\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b97e763-32c1-453d-8328-ec14bac52eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Loading the provided trainer model checkpoints. \n",
      "[Device 0]: Loading the 'tagger' torch model from the previously saved checkpoint: '/home/jupyter/warp-drive/tutorials/assets/tag_continuous_training/tagger_1000010000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'tagger' model to 1000010000. \n",
      "[Device 0]: Loading the 'runner' torch model from the previously saved checkpoint: '/home/jupyter/warp-drive/tutorials/assets/tag_continuous_training/runner_1000010000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'runner' model to 1000010000. \n"
     ]
    }
   ],
   "source": [
    "trainer.load_model_checkpoint(\n",
    "    {\n",
    "        \"tagger\": \"/home/jupyter/warp-drive/tutorials/assets/tag_continuous_training/tagger_1000010000.state_dict\",\n",
    "        \"runner\": \"/home/jupyter/warp-drive/tutorials/assets/tag_continuous_training/runner_1000010000.state_dict\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b80aa41",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/790665501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the entire episode roll-out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_tag_env_rollout_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1350\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Requested MovieWriter ({name}) not available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6232aa",
   "metadata": {},
   "source": [
    "After training, the runners learn to run away from the taggers, and the taggers learn to chase them; there are some instances where we see that taggers also team up to chase and tag the runners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd7bed",
   "metadata": {},
   "source": [
    "You have now seen how to train an entire multi-agent RL pipeline end-to-end. Please see the next [tutorial](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md) on scaling up training.\n",
    "\n",
    "We also have a [trainer](https://github.com/salesforce/warp-drive/blob/master/warp_drive/training/lightning_trainer.py) compatible with [Pytorch Lightning](https://www.pytorchlightning.ai/) and have prepared a tutorial on training with WarpDrive and Pytorch Lightning [here](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db85ef7",
   "metadata": {},
   "source": [
    "## Step 6: Gracefully shut down the trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49aa1f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Close the trainer to clear up the CUDA memory heap\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3b2c6",
   "metadata": {},
   "source": [
    "# Learn More and Explore our Tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337766a",
   "metadata": {},
   "source": [
    "For your reference, all our tutorials are here:\n",
    "1. [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "3. [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "4. [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "5. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "6. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "7. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.0__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
