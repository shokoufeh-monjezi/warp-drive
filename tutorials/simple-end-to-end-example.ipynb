{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started quickly with end-to-end multi-agent RL using WarpDrive! This shows a basic example to create a simple multi-agent Tag environment and get training. For more configuration options and indepth explanations, check out the other tutorials and source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"rl-warp-drive>=1.6.5\" \"torch==1.10.*\" \"torchvision==0.11.*\" \"torchtext==0.11.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pycuda/compyte/dtypes.py:120: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  reg.get_or_register_dtype(\"bool\", np.bool)\n"
     ]
    }
   ],
   "source": [
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "\n",
    "pytorch_cuda_init_success = torch.cuda.FloatTensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, Training, and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a set of run configurations for your experiments.\n",
    "# Note: these override some of the default configurations in 'warp_drive/training/run_configs/default_configs.yaml'.\n",
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     659.37\n",
      "Mean action sample time per iter (ms)   :      19.61\n",
      "Mean env. step time per iter (ms)       :      49.19\n",
      "Mean training time per iter (ms)        :      49.06\n",
      "Mean total time per iter (ms)           :     788.13\n",
      "Mean steps per sec (policy eval)        :   15165.90\n",
      "Mean steps per sec (action sample)      :  509980.94\n",
      "Mean steps per sec (env. step)          :  203306.12\n",
      "Mean steps per sec (training time)      :  203849.73\n",
      "Mean steps per sec (total)              :   12688.27\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.40319\n",
      "Policy loss                             :    0.64051\n",
      "Value function loss                     :    0.22058\n",
      "Mean rewards                            :    0.00126\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :   -0.00580\n",
      "Mean advantages                         :    0.13376\n",
      "Mean (norm.) advantages                 :    0.13376\n",
      "Mean (discounted) returns               :    0.12796\n",
      "Mean normalized returns                 :    0.12796\n",
      "Mean entropy                            :    4.79064\n",
      "Variance explained by the value function:   -0.00668\n",
      "Std. of action_0 over agents            :    3.17726\n",
      "Std. of action_0 over envs              :    3.19186\n",
      "Std. of action_0 over time              :    3.19290\n",
      "Std. of action_1 over agents            :    3.18294\n",
      "Std. of action_1 over envs              :    3.19679\n",
      "Std. of action_1 over time              :    3.19607\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.51000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.45999\n",
      "Policy loss                             :    1.69481\n",
      "Value function loss                     :    0.47435\n",
      "Mean rewards                            :    0.01740\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.10991\n",
      "Mean advantages                         :    0.35368\n",
      "Mean (norm.) advantages                 :    0.35368\n",
      "Mean (discounted) returns               :    0.46359\n",
      "Mean normalized returns                 :    0.46359\n",
      "Mean entropy                            :    4.79140\n",
      "Variance explained by the value function:   -0.00178\n",
      "Std. of action_0 over agents            :    3.03931\n",
      "Std. of action_0 over envs              :    3.16180\n",
      "Std. of action_0 over time              :    3.16070\n",
      "Std. of action_1 over agents            :    3.07494\n",
      "Std. of action_1 over envs              :    3.20101\n",
      "Std. of action_1 over time              :    3.20029\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.70000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1658962904/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1658962904/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     180.65\n",
      "Mean action sample time per iter (ms)   :      18.86\n",
      "Mean env. step time per iter (ms)       :      46.59\n",
      "Mean training time per iter (ms)        :      43.47\n",
      "Mean total time per iter (ms)           :     299.64\n",
      "Mean steps per sec (policy eval)        :   55355.10\n",
      "Mean steps per sec (action sample)      :  530342.37\n",
      "Mean steps per sec (env. step)          :  214637.94\n",
      "Mean steps per sec (training time)      :  230028.38\n",
      "Mean steps per sec (total)              :   33373.23\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.03655\n",
      "Policy loss                             :    0.20111\n",
      "Value function loss                     :    0.20593\n",
      "Mean rewards                            :    0.00064\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.05988\n",
      "Mean advantages                         :    0.04196\n",
      "Mean (norm.) advantages                 :    0.04196\n",
      "Mean (discounted) returns               :    0.10184\n",
      "Mean normalized returns                 :    0.10184\n",
      "Mean entropy                            :    4.79447\n",
      "Variance explained by the value function:    0.01941\n",
      "Std. of action_0 over agents            :    3.14526\n",
      "Std. of action_0 over envs              :    3.15999\n",
      "Std. of action_0 over time              :    3.15973\n",
      "Std. of action_1 over agents            :    3.12260\n",
      "Std. of action_1 over envs              :    3.13776\n",
      "Std. of action_1 over time              :    3.13908\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00598\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.37100\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.27745\n",
      "Policy loss                             :    0.51199\n",
      "Value function loss                     :    0.38844\n",
      "Mean rewards                            :    0.01858\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.40472\n",
      "Mean advantages                         :    0.10774\n",
      "Mean (norm.) advantages                 :    0.10774\n",
      "Mean (discounted) returns               :    0.51246\n",
      "Mean normalized returns                 :    0.51246\n",
      "Mean entropy                            :    4.76846\n",
      "Variance explained by the value function:    0.01313\n",
      "Std. of action_0 over agents            :    3.12435\n",
      "Std. of action_0 over envs              :    3.24142\n",
      "Std. of action_0 over time              :    3.24176\n",
      "Std. of action_1 over agents            :    2.91082\n",
      "Std. of action_1 over envs              :    3.02106\n",
      "Std. of action_1 over time              :    3.02036\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.03782\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.23300\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     169.34\n",
      "Mean action sample time per iter (ms)   :      19.86\n",
      "Mean env. step time per iter (ms)       :      48.45\n",
      "Mean training time per iter (ms)        :      47.01\n",
      "Mean total time per iter (ms)           :     294.98\n",
      "Mean steps per sec (policy eval)        :   59054.43\n",
      "Mean steps per sec (action sample)      :  503533.56\n",
      "Mean steps per sec (env. step)          :  206418.65\n",
      "Mean steps per sec (training time)      :  212727.82\n",
      "Mean steps per sec (total)              :   33900.79\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30967\n",
      "Policy loss                             :   -0.07202\n",
      "Value function loss                     :    0.19686\n",
      "Mean rewards                            :    0.00031\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11023\n",
      "Mean advantages                         :   -0.01497\n",
      "Mean (norm.) advantages                 :   -0.01497\n",
      "Mean (discounted) returns               :    0.09526\n",
      "Mean normalized returns                 :    0.09526\n",
      "Mean entropy                            :    4.79246\n",
      "Variance explained by the value function:    0.04244\n",
      "Std. of action_0 over agents            :    3.09577\n",
      "Std. of action_0 over envs              :    3.11122\n",
      "Std. of action_0 over time              :    3.11106\n",
      "Std. of action_1 over agents            :    3.15909\n",
      "Std. of action_1 over envs              :    3.17517\n",
      "Std. of action_1 over time              :    3.17419\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00593\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.63200\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.00943\n",
      "Policy loss                             :    0.22559\n",
      "Value function loss                     :    0.37263\n",
      "Mean rewards                            :    0.01924\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46378\n",
      "Mean advantages                         :    0.04754\n",
      "Mean (norm.) advantages                 :    0.04754\n",
      "Mean (discounted) returns               :    0.51132\n",
      "Mean normalized returns                 :    0.51132\n",
      "Mean entropy                            :    4.77474\n",
      "Variance explained by the value function:    0.03920\n",
      "Std. of action_0 over agents            :    3.07717\n",
      "Std. of action_0 over envs              :    3.20497\n",
      "Std. of action_0 over time              :    3.20528\n",
      "Std. of action_1 over agents            :    2.99440\n",
      "Std. of action_1 over envs              :    3.10825\n",
      "Std. of action_1 over time              :    3.10840\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.03136\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.59100\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     158.93\n",
      "Mean action sample time per iter (ms)   :      19.50\n",
      "Mean env. step time per iter (ms)       :      47.86\n",
      "Mean training time per iter (ms)        :      45.77\n",
      "Mean total time per iter (ms)           :     282.32\n",
      "Mean steps per sec (policy eval)        :   62919.00\n",
      "Mean steps per sec (action sample)      :  512883.96\n",
      "Mean steps per sec (env. step)          :  208946.39\n",
      "Mean steps per sec (training time)      :  218491.96\n",
      "Mean steps per sec (total)              :   35420.90\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.22946\n",
      "Policy loss                             :    0.00835\n",
      "Value function loss                     :    0.18359\n",
      "Mean rewards                            :    0.00063\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.10563\n",
      "Mean advantages                         :    0.00183\n",
      "Mean (norm.) advantages                 :    0.00183\n",
      "Mean (discounted) returns               :    0.10746\n",
      "Mean normalized returns                 :    0.10746\n",
      "Mean entropy                            :    4.79280\n",
      "Variance explained by the value function:    0.10195\n",
      "Std. of action_0 over agents            :    3.15415\n",
      "Std. of action_0 over envs              :    3.17059\n",
      "Std. of action_0 over time              :    3.17009\n",
      "Std. of action_1 over agents            :    3.17642\n",
      "Std. of action_1 over envs              :    3.19120\n",
      "Std. of action_1 over time              :    3.19083\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00622\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.34200\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.53121\n",
      "Policy loss                             :   -0.29624\n",
      "Value function loss                     :    0.37311\n",
      "Mean rewards                            :    0.01860\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.55114\n",
      "Mean advantages                         :   -0.06147\n",
      "Mean (norm.) advantages                 :   -0.06147\n",
      "Mean (discounted) returns               :    0.48967\n",
      "Mean normalized returns                 :    0.48967\n",
      "Mean entropy                            :    4.77394\n",
      "Variance explained by the value function:    0.07796\n",
      "Std. of action_0 over agents            :    3.03887\n",
      "Std. of action_0 over envs              :    3.16324\n",
      "Std. of action_0 over time              :    3.16741\n",
      "Std. of action_1 over agents            :    3.02203\n",
      "Std. of action_1 over envs              :    3.12958\n",
      "Std. of action_1 over time              :    3.12882\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02705\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.24900\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     157.26\n",
      "Mean action sample time per iter (ms)   :      19.70\n",
      "Mean env. step time per iter (ms)       :      48.18\n",
      "Mean training time per iter (ms)        :      46.60\n",
      "Mean total time per iter (ms)           :     282.09\n",
      "Mean steps per sec (policy eval)        :   63588.85\n",
      "Mean steps per sec (action sample)      :  507489.74\n",
      "Mean steps per sec (env. step)          :  207540.25\n",
      "Mean steps per sec (training time)      :  214605.77\n",
      "Mean steps per sec (total)              :   35449.71\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.26209\n",
      "Policy loss                             :   -0.02393\n",
      "Value function loss                     :    0.15298\n",
      "Mean rewards                            :    0.00037\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.10514\n",
      "Mean advantages                         :   -0.00493\n",
      "Mean (norm.) advantages                 :   -0.00493\n",
      "Mean (discounted) returns               :    0.10021\n",
      "Mean normalized returns                 :    0.10021\n",
      "Mean entropy                            :    4.79366\n",
      "Variance explained by the value function:    0.24977\n",
      "Std. of action_0 over agents            :    3.13254\n",
      "Std. of action_0 over envs              :    3.14880\n",
      "Std. of action_0 over time              :    3.14860\n",
      "Std. of action_1 over agents            :    3.14917\n",
      "Std. of action_1 over envs              :    3.16230\n",
      "Std. of action_1 over time              :    3.16204\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00589\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.05900\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.25027\n",
      "Policy loss                             :   -0.01513\n",
      "Value function loss                     :    0.36448\n",
      "Mean rewards                            :    0.01910\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.49805\n",
      "Mean advantages                         :   -0.00269\n",
      "Mean (norm.) advantages                 :   -0.00269\n",
      "Mean (discounted) returns               :    0.49536\n",
      "Mean normalized returns                 :    0.49536\n",
      "Mean entropy                            :    4.77558\n",
      "Variance explained by the value function:    0.09647\n",
      "Std. of action_0 over agents            :    3.04745\n",
      "Std. of action_0 over envs              :    3.16739\n",
      "Std. of action_0 over time              :    3.16871\n",
      "Std. of action_1 over agents            :    2.99225\n",
      "Std. of action_1 over envs              :    3.11686\n",
      "Std. of action_1 over time              :    3.11978\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02179\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.39300\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     154.51\n",
      "Mean action sample time per iter (ms)   :      19.62\n",
      "Mean env. step time per iter (ms)       :      48.06\n",
      "Mean training time per iter (ms)        :      46.52\n",
      "Mean total time per iter (ms)           :     279.02\n",
      "Mean steps per sec (policy eval)        :   64722.72\n",
      "Mean steps per sec (action sample)      :  509696.67\n",
      "Mean steps per sec (env. step)          :  208056.14\n",
      "Mean steps per sec (training time)      :  214982.92\n",
      "Mean steps per sec (total)              :   35839.33\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.15800\n",
      "Policy loss                             :    0.08026\n",
      "Value function loss                     :    0.13487\n",
      "Mean rewards                            :    0.00090\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.10102\n",
      "Mean advantages                         :    0.01680\n",
      "Mean (norm.) advantages                 :    0.01680\n",
      "Mean (discounted) returns               :    0.11782\n",
      "Mean normalized returns                 :    0.11782\n",
      "Mean entropy                            :    4.79222\n",
      "Variance explained by the value function:    0.33488\n",
      "Std. of action_0 over agents            :    3.09946\n",
      "Std. of action_0 over envs              :    3.11535\n",
      "Std. of action_0 over time              :    3.11566\n",
      "Std. of action_1 over agents            :    3.13749\n",
      "Std. of action_1 over envs              :    3.15343\n",
      "Std. of action_1 over time              :    3.15297\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00879\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.38667\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.09202\n",
      "Policy loss                             :    0.32751\n",
      "Value function loss                     :    0.33703\n",
      "Mean rewards                            :    0.01792\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.40017\n",
      "Mean advantages                         :    0.06903\n",
      "Mean (norm.) advantages                 :    0.06903\n",
      "Mean (discounted) returns               :    0.46920\n",
      "Mean normalized returns                 :    0.46920\n",
      "Mean entropy                            :    4.77727\n",
      "Variance explained by the value function:    0.10793\n",
      "Std. of action_0 over agents            :    3.04923\n",
      "Std. of action_0 over envs              :    3.16969\n",
      "Std. of action_0 over time              :    3.17170\n",
      "Std. of action_1 over agents            :    3.00646\n",
      "Std. of action_1 over envs              :    3.13560\n",
      "Std. of action_1 over time              :    3.13410\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02605\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.21444\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658962904/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1658962904/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1658962904/tagger_500000.state_dict'. \n",
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more and explore our tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about WarpDrive, take a look at these tutorials\n",
    "1. [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "3. [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "4. [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "5. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "6. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "7. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.0__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
