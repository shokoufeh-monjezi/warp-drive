{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started quickly with end-to-end multi-agent RL using WarpDrive! This shows a basic example to create a simple multi-agent Tag environment and get training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"rl-warp-drive>=1.6.5\" \"torch==1.10.*\" \"torchvision==0.11.*\" \"torchtext==0.11.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pycuda/compyte/dtypes.py:120: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  reg.get_or_register_dtype(\"bool\", np.bool)\n"
     ]
    }
   ],
   "source": [
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "\n",
    "pytorch_cuda_init_success = torch.cuda.FloatTensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, Training, and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a set of run configurations for your experiments.\n",
    "# Note: these override some of the default configurations in 'warp_drive/training/run_configs/default_configs.yaml'.\n",
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     770.28\n",
      "Mean action sample time per iter (ms)   :      21.13\n",
      "Mean env. step time per iter (ms)       :     335.73\n",
      "Mean training time per iter (ms)        :     918.82\n",
      "Mean total time per iter (ms)           :    2057.39\n",
      "Mean steps per sec (policy eval)        :   12982.27\n",
      "Mean steps per sec (action sample)      :  473370.09\n",
      "Mean steps per sec (env. step)          :   29785.51\n",
      "Mean steps per sec (training time)      :   10883.47\n",
      "Mean steps per sec (total)              :    4860.52\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.06799\n",
      "Policy loss                             :    0.30557\n",
      "Value function loss                     :    0.20517\n",
      "Mean rewards                            :    0.00107\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.05543\n",
      "Mean advantages                         :    0.06377\n",
      "Mean (norm.) advantages                 :    0.06377\n",
      "Mean (discounted) returns               :    0.11920\n",
      "Mean normalized returns                 :    0.11920\n",
      "Mean entropy                            :    4.79273\n",
      "Variance explained by the value function:    0.01418\n",
      "Std. of action_0 over agents            :    3.16788\n",
      "Std. of action_0 over envs              :    3.18205\n",
      "Std. of action_0 over time              :    3.18131\n",
      "Std. of action_1 over agents            :    3.11355\n",
      "Std. of action_1 over envs              :    3.12902\n",
      "Std. of action_1 over time              :    3.13052\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.15000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.46833\n",
      "Policy loss                             :    2.70100\n",
      "Value function loss                     :    0.68673\n",
      "Mean rewards                            :    0.01768\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   -0.08282\n",
      "Mean advantages                         :    0.56384\n",
      "Mean (norm.) advantages                 :    0.56384\n",
      "Mean (discounted) returns               :    0.48102\n",
      "Mean normalized returns                 :    0.48102\n",
      "Mean entropy                            :    4.79058\n",
      "Variance explained by the value function:   -0.00252\n",
      "Std. of action_0 over agents            :    3.04610\n",
      "Std. of action_0 over envs              :    3.15061\n",
      "Std. of action_0 over time              :    3.15203\n",
      "Std. of action_1 over agents            :    3.03028\n",
      "Std. of action_1 over envs              :    3.14957\n",
      "Std. of action_1 over time              :    3.14849\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.84000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1659121787/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1659121787/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     216.70\n",
      "Mean action sample time per iter (ms)   :      20.88\n",
      "Mean env. step time per iter (ms)       :      76.85\n",
      "Mean training time per iter (ms)        :     128.31\n",
      "Mean total time per iter (ms)           :     453.82\n",
      "Mean steps per sec (policy eval)        :   46146.82\n",
      "Mean steps per sec (action sample)      :  479016.70\n",
      "Mean steps per sec (env. step)          :  130120.31\n",
      "Mean steps per sec (training time)      :   77933.54\n",
      "Mean steps per sec (total)              :   22035.16\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.39129\n",
      "Policy loss                             :   -0.15365\n",
      "Value function loss                     :    0.20179\n",
      "Mean rewards                            :   -0.00005\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11429\n",
      "Mean advantages                         :   -0.03203\n",
      "Mean (norm.) advantages                 :   -0.03203\n",
      "Mean (discounted) returns               :    0.08226\n",
      "Mean normalized returns                 :    0.08226\n",
      "Mean entropy                            :    4.79326\n",
      "Variance explained by the value function:    0.03584\n",
      "Std. of action_0 over agents            :    3.16746\n",
      "Std. of action_0 over envs              :    3.18150\n",
      "Std. of action_0 over time              :    3.18198\n",
      "Std. of action_1 over agents            :    3.15889\n",
      "Std. of action_1 over envs              :    3.17413\n",
      "Std. of action_1 over time              :    3.17358\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00834\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.40800\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.69308\n",
      "Policy loss                             :    0.92708\n",
      "Value function loss                     :    0.45620\n",
      "Mean rewards                            :    0.01982\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.33259\n",
      "Mean advantages                         :    0.19482\n",
      "Mean (norm.) advantages                 :    0.19482\n",
      "Mean (discounted) returns               :    0.52741\n",
      "Mean normalized returns                 :    0.52741\n",
      "Mean entropy                            :    4.77126\n",
      "Variance explained by the value function:   -0.00166\n",
      "Std. of action_0 over agents            :    3.10615\n",
      "Std. of action_0 over envs              :    3.22454\n",
      "Std. of action_0 over time              :    3.22508\n",
      "Std. of action_1 over agents            :    2.96615\n",
      "Std. of action_1 over envs              :    3.09034\n",
      "Std. of action_1 over time              :    3.08971\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.04256\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.21700\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     190.96\n",
      "Mean action sample time per iter (ms)   :      20.86\n",
      "Mean env. step time per iter (ms)       :      64.50\n",
      "Mean training time per iter (ms)        :      90.98\n",
      "Mean total time per iter (ms)           :     378.40\n",
      "Mean steps per sec (policy eval)        :   52368.10\n",
      "Mean steps per sec (action sample)      :  479343.81\n",
      "Mean steps per sec (env. step)          :  155028.03\n",
      "Mean steps per sec (training time)      :  109910.25\n",
      "Mean steps per sec (total)              :   26427.38\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.33406\n",
      "Policy loss                             :   -0.09626\n",
      "Value function loss                     :    0.18732\n",
      "Mean rewards                            :    0.00056\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.12580\n",
      "Mean advantages                         :   -0.02001\n",
      "Mean (norm.) advantages                 :   -0.02001\n",
      "Mean (discounted) returns               :    0.10579\n",
      "Mean normalized returns                 :    0.10579\n",
      "Mean entropy                            :    4.79352\n",
      "Variance explained by the value function:    0.08254\n",
      "Std. of action_0 over agents            :    3.11465\n",
      "Std. of action_0 over envs              :    3.12996\n",
      "Std. of action_0 over time              :    3.13051\n",
      "Std. of action_1 over agents            :    3.16425\n",
      "Std. of action_1 over envs              :    3.18023\n",
      "Std. of action_1 over time              :    3.17925\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00549\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.01300\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.41190\n",
      "Policy loss                             :   -0.17774\n",
      "Value function loss                     :    0.38900\n",
      "Mean rewards                            :    0.01870\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.52748\n",
      "Mean advantages                         :   -0.03679\n",
      "Mean (norm.) advantages                 :   -0.03679\n",
      "Mean (discounted) returns               :    0.49069\n",
      "Mean normalized returns                 :    0.49069\n",
      "Mean entropy                            :    4.76092\n",
      "Variance explained by the value function:    0.01166\n",
      "Std. of action_0 over agents            :    3.09150\n",
      "Std. of action_0 over envs              :    3.21352\n",
      "Std. of action_0 over time              :    3.21221\n",
      "Std. of action_1 over agents            :    2.98356\n",
      "Std. of action_1 over envs              :    3.10055\n",
      "Std. of action_1 over time              :    3.10089\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.02471\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.39700\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     182.08\n",
      "Mean action sample time per iter (ms)   :      20.87\n",
      "Mean env. step time per iter (ms)       :      60.19\n",
      "Mean training time per iter (ms)        :      77.64\n",
      "Mean total time per iter (ms)           :     351.89\n",
      "Mean steps per sec (policy eval)        :   54922.03\n",
      "Mean steps per sec (action sample)      :  479170.80\n",
      "Mean steps per sec (env. step)          :  166134.12\n",
      "Mean steps per sec (training time)      :  128791.52\n",
      "Mean steps per sec (total)              :   28418.19\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.29704\n",
      "Policy loss                             :   -0.05899\n",
      "Value function loss                     :    0.16023\n",
      "Mean rewards                            :    0.00056\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11719\n",
      "Mean advantages                         :   -0.01224\n",
      "Mean (norm.) advantages                 :   -0.01224\n",
      "Mean (discounted) returns               :    0.10495\n",
      "Mean normalized returns                 :    0.10495\n",
      "Mean entropy                            :    4.79304\n",
      "Variance explained by the value function:    0.21782\n",
      "Std. of action_0 over agents            :    3.11922\n",
      "Std. of action_0 over envs              :    3.13312\n",
      "Std. of action_0 over time              :    3.13239\n",
      "Std. of action_1 over agents            :    3.15764\n",
      "Std. of action_1 over envs              :    3.17379\n",
      "Std. of action_1 over time              :    3.17446\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00615\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.19600\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.12530\n",
      "Policy loss                             :    0.10963\n",
      "Value function loss                     :    0.37604\n",
      "Mean rewards                            :    0.01864\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46850\n",
      "Mean advantages                         :    0.02310\n",
      "Mean (norm.) advantages                 :    0.02310\n",
      "Mean (discounted) returns               :    0.49160\n",
      "Mean normalized returns                 :    0.49160\n",
      "Mean entropy                            :    4.77371\n",
      "Variance explained by the value function:    0.03722\n",
      "Std. of action_0 over agents            :    3.08326\n",
      "Std. of action_0 over envs              :    3.19835\n",
      "Std. of action_0 over time              :    3.19876\n",
      "Std. of action_1 over agents            :    2.98430\n",
      "Std. of action_1 over envs              :    3.10407\n",
      "Std. of action_1 over time              :    3.10472\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02565\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.31200\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     178.10\n",
      "Mean action sample time per iter (ms)   :      20.96\n",
      "Mean env. step time per iter (ms)       :      58.05\n",
      "Mean training time per iter (ms)        :      71.27\n",
      "Mean total time per iter (ms)           :     339.56\n",
      "Mean steps per sec (policy eval)        :   56148.50\n",
      "Mean steps per sec (action sample)      :  477116.70\n",
      "Mean steps per sec (env. step)          :  172260.96\n",
      "Mean steps per sec (training time)      :  140301.66\n",
      "Mean steps per sec (total)              :   29449.72\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.35693\n",
      "Policy loss                             :   -0.11870\n",
      "Value function loss                     :    0.14076\n",
      "Mean rewards                            :    0.00038\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.12204\n",
      "Mean advantages                         :   -0.02472\n",
      "Mean (norm.) advantages                 :   -0.02472\n",
      "Mean (discounted) returns               :    0.09732\n",
      "Mean normalized returns                 :    0.09732\n",
      "Mean entropy                            :    4.79293\n",
      "Variance explained by the value function:    0.32054\n",
      "Std. of action_0 over agents            :    3.12077\n",
      "Std. of action_0 over envs              :    3.13614\n",
      "Std. of action_0 over time              :    3.13616\n",
      "Std. of action_1 over agents            :    3.15252\n",
      "Std. of action_1 over envs              :    3.16898\n",
      "Std. of action_1 over time              :    3.16919\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00519\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.80400\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.23289\n",
      "Policy loss                             :    0.00212\n",
      "Value function loss                     :    0.36113\n",
      "Mean rewards                            :    0.01894\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.50335\n",
      "Mean advantages                         :    0.00082\n",
      "Mean (norm.) advantages                 :    0.00082\n",
      "Mean (discounted) returns               :    0.50417\n",
      "Mean normalized returns                 :    0.50417\n",
      "Mean entropy                            :    4.77234\n",
      "Variance explained by the value function:    0.07559\n",
      "Std. of action_0 over agents            :    3.09593\n",
      "Std. of action_0 over envs              :    3.21607\n",
      "Std. of action_0 over time              :    3.21576\n",
      "Std. of action_1 over agents            :    2.93614\n",
      "Std. of action_1 over envs              :    3.04940\n",
      "Std. of action_1 over time              :    3.04916\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02625\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.47300\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     175.43\n",
      "Mean action sample time per iter (ms)   :      20.96\n",
      "Mean env. step time per iter (ms)       :      56.82\n",
      "Mean training time per iter (ms)        :      67.32\n",
      "Mean total time per iter (ms)           :     331.71\n",
      "Mean steps per sec (policy eval)        :   57002.69\n",
      "Mean steps per sec (action sample)      :  477120.45\n",
      "Mean steps per sec (env. step)          :  176000.14\n",
      "Mean steps per sec (training time)      :  148551.47\n",
      "Mean steps per sec (total)              :   30147.17\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.34139\n",
      "Policy loss                             :   -0.10305\n",
      "Value function loss                     :    0.13197\n",
      "Mean rewards                            :    0.00071\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.13262\n",
      "Mean advantages                         :   -0.02145\n",
      "Mean (norm.) advantages                 :   -0.02145\n",
      "Mean (discounted) returns               :    0.11117\n",
      "Mean normalized returns                 :    0.11117\n",
      "Mean entropy                            :    4.79320\n",
      "Variance explained by the value function:    0.35046\n",
      "Std. of action_0 over agents            :    3.12240\n",
      "Std. of action_0 over envs              :    3.13708\n",
      "Std. of action_0 over time              :    3.13741\n",
      "Std. of action_1 over agents            :    3.17452\n",
      "Std. of action_1 over envs              :    3.18831\n",
      "Std. of action_1 over time              :    3.18966\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00965\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.62778\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.34445\n",
      "Policy loss                             :   -0.10912\n",
      "Value function loss                     :    0.33659\n",
      "Mean rewards                            :    0.01852\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.50614\n",
      "Mean advantages                         :   -0.02229\n",
      "Mean (norm.) advantages                 :   -0.02229\n",
      "Mean (discounted) returns               :    0.48385\n",
      "Mean normalized returns                 :    0.48385\n",
      "Mean entropy                            :    4.77404\n",
      "Variance explained by the value function:    0.10617\n",
      "Std. of action_0 over agents            :    3.06427\n",
      "Std. of action_0 over envs              :    3.18101\n",
      "Std. of action_0 over time              :    3.18085\n",
      "Std. of action_1 over agents            :    2.96155\n",
      "Std. of action_1 over envs              :    3.07556\n",
      "Std. of action_1 over time              :    3.07427\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02142\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.10889\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1659121787/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1659121787/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1659121787/tagger_500000.state_dict'. \n",
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more and explore our tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about WarpDrive, take a look at these tutorials\n",
    "1. [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "3. [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "4. [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "5. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "6. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "7. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.0__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
