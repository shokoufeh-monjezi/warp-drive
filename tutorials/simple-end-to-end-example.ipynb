{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started quickly with end-to-end multi-agent RL using WarpDrive! This shows a basic example to create a simple multi-agent Tag environment and get training. For more configuration options and indepth explanations, check out the other tutorials and source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --quiet \"rl-warp-drive>=1.6.5\" \"torch==1.10.*\" \"torchvision==0.11.*\" \"torchtext==0.11.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after pip install warp-drive we need restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "\n",
    "pytorch_cuda_init_success = torch.cuda.FloatTensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, Training, and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a set of run configurations for your experiments.\n",
    "# Note: these override some of the default configurations in 'warp_drive/training/run_configs/default_configs.yaml'.\n",
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     759.20\n",
      "Mean action sample time per iter (ms)   :      22.55\n",
      "Mean env. step time per iter (ms)       :      53.69\n",
      "Mean training time per iter (ms)        :      56.01\n",
      "Mean total time per iter (ms)           :     903.60\n",
      "Mean steps per sec (policy eval)        :   13171.69\n",
      "Mean steps per sec (action sample)      :  443549.30\n",
      "Mean steps per sec (env. step)          :  186243.82\n",
      "Mean steps per sec (training time)      :  178530.63\n",
      "Mean steps per sec (total)              :   11066.84\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.66867\n",
      "Policy loss                             :    0.90586\n",
      "Value function loss                     :    0.23846\n",
      "Mean rewards                            :    0.00091\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :   -0.07341\n",
      "Mean advantages                         :    0.18910\n",
      "Mean (norm.) advantages                 :    0.18910\n",
      "Mean (discounted) returns               :    0.11569\n",
      "Mean normalized returns                 :    0.11569\n",
      "Mean entropy                            :    4.79157\n",
      "Variance explained by the value function:    0.00333\n",
      "Std. of action_0 over agents            :    3.10519\n",
      "Std. of action_0 over envs              :    3.12040\n",
      "Std. of action_0 over time              :    3.11999\n",
      "Std. of action_1 over agents            :    3.14275\n",
      "Std. of action_1 over envs              :    3.15883\n",
      "Std. of action_1 over time              :    3.15835\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.83000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    2.57574\n",
      "Policy loss                             :    2.80850\n",
      "Value function loss                     :    0.69304\n",
      "Mean rewards                            :    0.01798\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :   -0.10589\n",
      "Mean advantages                         :    0.58583\n",
      "Mean (norm.) advantages                 :    0.58583\n",
      "Mean (discounted) returns               :    0.47994\n",
      "Mean normalized returns                 :    0.47994\n",
      "Mean entropy                            :    4.79390\n",
      "Variance explained by the value function:    0.00285\n",
      "Std. of action_0 over agents            :    3.05880\n",
      "Std. of action_0 over envs              :    3.17173\n",
      "Std. of action_0 over time              :    3.16825\n",
      "Std. of action_1 over agents            :    3.04183\n",
      "Std. of action_1 over envs              :    3.16258\n",
      "Std. of action_1 over time              :    3.16288\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.99000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1658873671/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1658873671/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     211.40\n",
      "Mean action sample time per iter (ms)   :      20.98\n",
      "Mean env. step time per iter (ms)       :      50.80\n",
      "Mean training time per iter (ms)        :      48.08\n",
      "Mean total time per iter (ms)           :     342.57\n",
      "Mean steps per sec (policy eval)        :   47303.58\n",
      "Mean steps per sec (action sample)      :  476724.94\n",
      "Mean steps per sec (env. step)          :  196835.26\n",
      "Mean steps per sec (training time)      :  208003.75\n",
      "Mean steps per sec (total)              :   29191.50\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.18581\n",
      "Policy loss                             :    0.42344\n",
      "Value function loss                     :    0.20942\n",
      "Mean rewards                            :    0.00048\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.01325\n",
      "Mean advantages                         :    0.08832\n",
      "Mean (norm.) advantages                 :    0.08832\n",
      "Mean (discounted) returns               :    0.10157\n",
      "Mean normalized returns                 :    0.10157\n",
      "Mean entropy                            :    4.79447\n",
      "Variance explained by the value function:    0.01798\n",
      "Std. of action_0 over agents            :    3.11378\n",
      "Std. of action_0 over envs              :    3.12761\n",
      "Std. of action_0 over time              :    3.12799\n",
      "Std. of action_1 over agents            :    3.14669\n",
      "Std. of action_1 over envs              :    3.16041\n",
      "Std. of action_1 over time              :    3.16090\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00880\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.43000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.17812\n",
      "Policy loss                             :    0.41308\n",
      "Value function loss                     :    0.40765\n",
      "Mean rewards                            :    0.01886\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.41224\n",
      "Mean advantages                         :    0.08683\n",
      "Mean (norm.) advantages                 :    0.08683\n",
      "Mean (discounted) returns               :    0.49907\n",
      "Mean normalized returns                 :    0.49907\n",
      "Mean entropy                            :    4.78068\n",
      "Variance explained by the value function:   -0.00279\n",
      "Std. of action_0 over agents            :    3.08018\n",
      "Std. of action_0 over envs              :    3.19913\n",
      "Std. of action_0 over time              :    3.20056\n",
      "Std. of action_1 over agents            :    3.00197\n",
      "Std. of action_1 over envs              :    3.12958\n",
      "Std. of action_1 over time              :    3.12853\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.02415\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.20300\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     185.88\n",
      "Mean action sample time per iter (ms)   :      21.00\n",
      "Mean env. step time per iter (ms)       :      50.59\n",
      "Mean training time per iter (ms)        :      47.87\n",
      "Mean total time per iter (ms)           :     316.63\n",
      "Mean steps per sec (policy eval)        :   53799.56\n",
      "Mean steps per sec (action sample)      :  476203.75\n",
      "Mean steps per sec (env. step)          :  197673.05\n",
      "Mean steps per sec (training time)      :  208914.63\n",
      "Mean steps per sec (total)              :   31582.24\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.42841\n",
      "Policy loss                             :   -0.19083\n",
      "Value function loss                     :    0.19925\n",
      "Mean rewards                            :    0.00079\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.14794\n",
      "Mean advantages                         :   -0.03978\n",
      "Mean (norm.) advantages                 :   -0.03978\n",
      "Mean (discounted) returns               :    0.10816\n",
      "Mean normalized returns                 :    0.10816\n",
      "Mean entropy                            :    4.79158\n",
      "Variance explained by the value function:    0.04255\n",
      "Std. of action_0 over agents            :    3.12815\n",
      "Std. of action_0 over envs              :    3.14148\n",
      "Std. of action_0 over time              :    3.14285\n",
      "Std. of action_1 over agents            :    3.14881\n",
      "Std. of action_1 over envs              :    3.16454\n",
      "Std. of action_1 over time              :    3.16334\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00709\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.32300\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.25009\n",
      "Policy loss                             :   -0.01520\n",
      "Value function loss                     :    0.38201\n",
      "Mean rewards                            :    0.01834\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.50578\n",
      "Mean advantages                         :   -0.00298\n",
      "Mean (norm.) advantages                 :   -0.00298\n",
      "Mean (discounted) returns               :    0.50280\n",
      "Mean normalized returns                 :    0.50280\n",
      "Mean entropy                            :    4.77432\n",
      "Variance explained by the value function:    0.02112\n",
      "Std. of action_0 over agents            :    3.04319\n",
      "Std. of action_0 over envs              :    3.17405\n",
      "Std. of action_0 over time              :    3.17643\n",
      "Std. of action_1 over agents            :    3.00555\n",
      "Std. of action_1 over envs              :    3.12323\n",
      "Std. of action_1 over time              :    3.12490\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.02569\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.27200\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     176.42\n",
      "Mean action sample time per iter (ms)   :      20.92\n",
      "Mean env. step time per iter (ms)       :      50.50\n",
      "Mean training time per iter (ms)        :      47.63\n",
      "Mean total time per iter (ms)           :     306.75\n",
      "Mean steps per sec (policy eval)        :   56684.01\n",
      "Mean steps per sec (action sample)      :  478005.67\n",
      "Mean steps per sec (env. step)          :  198033.11\n",
      "Mean steps per sec (training time)      :  209951.39\n",
      "Mean steps per sec (total)              :   32599.77\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13828\n",
      "Policy loss                             :    0.09954\n",
      "Value function loss                     :    0.18802\n",
      "Mean rewards                            :    0.00108\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.09957\n",
      "Mean advantages                         :    0.02084\n",
      "Mean (norm.) advantages                 :    0.02084\n",
      "Mean (discounted) returns               :    0.12041\n",
      "Mean normalized returns                 :    0.12041\n",
      "Mean entropy                            :    4.79390\n",
      "Variance explained by the value function:    0.07704\n",
      "Std. of action_0 over agents            :    3.11491\n",
      "Std. of action_0 over envs              :    3.12774\n",
      "Std. of action_0 over time              :    3.12924\n",
      "Std. of action_1 over agents            :    3.16440\n",
      "Std. of action_1 over envs              :    3.17911\n",
      "Std. of action_1 over time              :    3.17902\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00705\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.48200\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.09716\n",
      "Policy loss                             :    0.13818\n",
      "Value function loss                     :    0.36282\n",
      "Mean rewards                            :    0.01770\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.44983\n",
      "Mean advantages                         :    0.02937\n",
      "Mean (norm.) advantages                 :    0.02937\n",
      "Mean (discounted) returns               :    0.47919\n",
      "Mean normalized returns                 :    0.47919\n",
      "Mean entropy                            :    4.77924\n",
      "Variance explained by the value function:    0.05000\n",
      "Std. of action_0 over agents            :    3.08829\n",
      "Std. of action_0 over envs              :    3.20688\n",
      "Std. of action_0 over time              :    3.20719\n",
      "Std. of action_1 over agents            :    2.99006\n",
      "Std. of action_1 over envs              :    3.10531\n",
      "Std. of action_1 over time              :    3.10667\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02683\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.17500\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     171.73\n",
      "Mean action sample time per iter (ms)   :      20.88\n",
      "Mean env. step time per iter (ms)       :      50.43\n",
      "Mean training time per iter (ms)        :      47.50\n",
      "Mean total time per iter (ms)           :     301.82\n",
      "Mean steps per sec (policy eval)        :   58231.63\n",
      "Mean steps per sec (action sample)      :  478887.50\n",
      "Mean steps per sec (env. step)          :  198294.06\n",
      "Mean steps per sec (training time)      :  210534.67\n",
      "Mean steps per sec (total)              :   33131.89\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.13491\n",
      "Policy loss                             :    0.10304\n",
      "Value function loss                     :    0.17783\n",
      "Mean rewards                            :    0.00064\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.08341\n",
      "Mean advantages                         :    0.02154\n",
      "Mean (norm.) advantages                 :    0.02154\n",
      "Mean (discounted) returns               :    0.10495\n",
      "Mean normalized returns                 :    0.10495\n",
      "Mean entropy                            :    4.79447\n",
      "Variance explained by the value function:    0.14145\n",
      "Std. of action_0 over agents            :    3.13829\n",
      "Std. of action_0 over envs              :    3.15366\n",
      "Std. of action_0 over time              :    3.15424\n",
      "Std. of action_1 over agents            :    3.14981\n",
      "Std. of action_1 over envs              :    3.16444\n",
      "Std. of action_1 over time              :    3.16431\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00697\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.04200\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.32749\n",
      "Policy loss                             :   -0.09241\n",
      "Value function loss                     :    0.35152\n",
      "Mean rewards                            :    0.01846\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.51411\n",
      "Mean advantages                         :   -0.01879\n",
      "Mean (norm.) advantages                 :   -0.01879\n",
      "Mean (discounted) returns               :    0.49532\n",
      "Mean normalized returns                 :    0.49532\n",
      "Mean entropy                            :    4.77198\n",
      "Variance explained by the value function:    0.08489\n",
      "Std. of action_0 over agents            :    3.11834\n",
      "Std. of action_0 over envs              :    3.23504\n",
      "Std. of action_0 over time              :    3.23689\n",
      "Std. of action_1 over agents            :    2.97105\n",
      "Std. of action_1 over envs              :    3.09073\n",
      "Std. of action_1 over time              :    3.09028\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02117\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.38100\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     169.30\n",
      "Mean action sample time per iter (ms)   :      20.90\n",
      "Mean env. step time per iter (ms)       :      50.41\n",
      "Mean training time per iter (ms)        :      47.51\n",
      "Mean total time per iter (ms)           :     299.42\n",
      "Mean steps per sec (policy eval)        :   59068.04\n",
      "Mean steps per sec (action sample)      :  478504.49\n",
      "Mean steps per sec (env. step)          :  198354.47\n",
      "Mean steps per sec (training time)      :  210462.34\n",
      "Mean steps per sec (total)              :   33397.86\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14271\n",
      "Policy loss                             :    0.09544\n",
      "Value function loss                     :    0.14784\n",
      "Mean rewards                            :    0.00078\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.09245\n",
      "Mean advantages                         :    0.01999\n",
      "Mean (norm.) advantages                 :    0.01999\n",
      "Mean (discounted) returns               :    0.11244\n",
      "Mean normalized returns                 :    0.11244\n",
      "Mean entropy                            :    4.79252\n",
      "Variance explained by the value function:    0.27797\n",
      "Std. of action_0 over agents            :    3.15352\n",
      "Std. of action_0 over envs              :    3.16799\n",
      "Std. of action_0 over time              :    3.16813\n",
      "Std. of action_1 over agents            :    3.18619\n",
      "Std. of action_1 over envs              :    3.19821\n",
      "Std. of action_1 over time              :    3.19946\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00784\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.98222\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.14668\n",
      "Policy loss                             :    0.08850\n",
      "Value function loss                     :    0.33894\n",
      "Mean rewards                            :    0.01822\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46246\n",
      "Mean advantages                         :    0.01901\n",
      "Mean (norm.) advantages                 :    0.01901\n",
      "Mean (discounted) returns               :    0.48146\n",
      "Mean normalized returns                 :    0.48146\n",
      "Mean entropy                            :    4.77133\n",
      "Variance explained by the value function:    0.12168\n",
      "Std. of action_0 over agents            :    3.07181\n",
      "Std. of action_0 over envs              :    3.19030\n",
      "Std. of action_0 over time              :    3.18856\n",
      "Std. of action_1 over agents            :    2.94363\n",
      "Std. of action_1 over envs              :    3.05342\n",
      "Std. of action_1 over time              :    3.05495\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02159\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.40444\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1658873671/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1658873671/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1658873671/tagger_500000.state_dict'. \n",
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more and explore our tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about WarpDrive, take a look at these tutorials\n",
    "1. [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "3. [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "4. [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "5. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "6. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "7. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.0__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
