{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started quickly with end-to-end multi-agent RL using WarpDrive! This shows a basic example to create a simple multi-agent Tag environment and get training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"rl-warp-drive>=1.6.5\" \"torch==1.10.*\" \"torchvision==0.11.*\" \"torchtext==0.11.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pycuda/compyte/dtypes.py:120: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  reg.get_or_register_dtype(\"bool\", np.bool)\n"
     ]
    }
   ],
   "source": [
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "\n",
    "pytorch_cuda_init_success = torch.cuda.FloatTensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, Training, and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a set of run configurations for your experiments.\n",
    "# Note: these override some of the default configurations in 'warp_drive/training/run_configs/default_configs.yaml'.\n",
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings\n",
    "    env=dict(\n",
    "        num_taggers=5,\n",
    "        num_runners=20,\n",
    "        episode_length=100,\n",
    "        seed=1234,\n",
    "        use_full_observation=False,\n",
    "        num_other_agents_observed=10,\n",
    "        tagging_distance=0.02,\n",
    "    ),\n",
    "    # Trainer settings\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.002,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting\n",
    "    saving=dict(\n",
    "        metrics_log_freq=10,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     736.01\n",
      "Mean action sample time per iter (ms)   :      20.47\n",
      "Mean env. step time per iter (ms)       :      49.93\n",
      "Mean training time per iter (ms)        :      52.36\n",
      "Mean total time per iter (ms)           :     869.70\n",
      "Mean steps per sec (policy eval)        :   13586.76\n",
      "Mean steps per sec (action sample)      :  488403.35\n",
      "Mean steps per sec (env. step)          :  200300.23\n",
      "Mean steps per sec (training time)      :  190969.84\n",
      "Mean steps per sec (total)              :   11498.16\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.82353\n",
      "Policy loss                             :    1.06061\n",
      "Value function loss                     :    0.25759\n",
      "Mean rewards                            :    0.00146\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :   -0.08903\n",
      "Mean advantages                         :    0.22132\n",
      "Mean (norm.) advantages                 :    0.22132\n",
      "Mean (discounted) returns               :    0.13230\n",
      "Mean normalized returns                 :    0.13230\n",
      "Mean entropy                            :    4.79331\n",
      "Variance explained by the value function:   -0.03035\n",
      "Std. of action_0 over agents            :    3.12958\n",
      "Std. of action_0 over envs              :    3.14613\n",
      "Std. of action_0 over time              :    3.14639\n",
      "Std. of action_1 over agents            :    3.12220\n",
      "Std. of action_1 over envs              :    3.13740\n",
      "Std. of action_1 over time              :    3.13706\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    2.91000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    1.87671\n",
      "Policy loss                             :    2.11070\n",
      "Value function loss                     :    0.56535\n",
      "Mean rewards                            :    0.01688\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.01956\n",
      "Mean advantages                         :    0.44037\n",
      "Mean (norm.) advantages                 :    0.44037\n",
      "Mean (discounted) returns               :    0.45993\n",
      "Mean normalized returns                 :    0.45993\n",
      "Mean entropy                            :    4.79278\n",
      "Variance explained by the value function:   -0.00492\n",
      "Std. of action_0 over agents            :    3.06935\n",
      "Std. of action_0 over envs              :    3.18025\n",
      "Std. of action_0 over time              :    3.18204\n",
      "Std. of action_1 over agents            :    3.08759\n",
      "Std. of action_1 over envs              :    3.20371\n",
      "Std. of action_1 over time              :    3.20605\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    8.44000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1661375510/runner_10000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1661375510/tagger_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 11 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     206.46\n",
      "Mean action sample time per iter (ms)   :      20.15\n",
      "Mean env. step time per iter (ms)       :      49.66\n",
      "Mean training time per iter (ms)        :      46.12\n",
      "Mean total time per iter (ms)           :     333.16\n",
      "Mean steps per sec (policy eval)        :   48436.01\n",
      "Mean steps per sec (action sample)      :  496326.46\n",
      "Mean steps per sec (env. step)          :  201370.43\n",
      "Mean steps per sec (training time)      :  216841.73\n",
      "Mean steps per sec (total)              :   30015.46\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.04470\n",
      "Policy loss                             :    0.28238\n",
      "Value function loss                     :    0.20163\n",
      "Mean rewards                            :    0.00094\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.05848\n",
      "Mean advantages                         :    0.05893\n",
      "Mean (norm.) advantages                 :    0.05893\n",
      "Mean (discounted) returns               :    0.11741\n",
      "Mean normalized returns                 :    0.11741\n",
      "Mean entropy                            :    4.79410\n",
      "Variance explained by the value function:    0.02677\n",
      "Std. of action_0 over agents            :    3.12425\n",
      "Std. of action_0 over envs              :    3.13851\n",
      "Std. of action_0 over time              :    3.13692\n",
      "Std. of action_1 over agents            :    3.15798\n",
      "Std. of action_1 over envs              :    3.17221\n",
      "Std. of action_1 over time              :    3.17324\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.00644\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.77400\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :    0.10891\n",
      "Policy loss                             :    0.34407\n",
      "Value function loss                     :    0.36455\n",
      "Mean rewards                            :    0.01780\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.40038\n",
      "Mean advantages                         :    0.07240\n",
      "Mean (norm.) advantages                 :    0.07240\n",
      "Mean (discounted) returns               :    0.47278\n",
      "Mean normalized returns                 :    0.47278\n",
      "Mean entropy                            :    4.77604\n",
      "Variance explained by the value function:    0.00333\n",
      "Std. of action_0 over agents            :    3.06475\n",
      "Std. of action_0 over envs              :    3.19402\n",
      "Std. of action_0 over time              :    3.19613\n",
      "Std. of action_1 over agents            :    2.95788\n",
      "Std. of action_1 over envs              :    3.06791\n",
      "Std. of action_1 over time              :    3.06862\n",
      "Current timestep                        : 110000.00000\n",
      "Gradient norm                           :    0.02961\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.03000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 21 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     179.70\n",
      "Mean action sample time per iter (ms)   :      19.82\n",
      "Mean env. step time per iter (ms)       :      49.31\n",
      "Mean training time per iter (ms)        :      45.70\n",
      "Mean total time per iter (ms)           :     305.20\n",
      "Mean steps per sec (policy eval)        :   55646.99\n",
      "Mean steps per sec (action sample)      :  504509.76\n",
      "Mean steps per sec (env. step)          :  202797.04\n",
      "Mean steps per sec (training time)      :  218815.35\n",
      "Mean steps per sec (total)              :   32765.02\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20992\n",
      "Policy loss                             :    0.02785\n",
      "Value function loss                     :    0.19176\n",
      "Mean rewards                            :    0.00079\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.10625\n",
      "Mean advantages                         :    0.00584\n",
      "Mean (norm.) advantages                 :    0.00584\n",
      "Mean (discounted) returns               :    0.11209\n",
      "Mean normalized returns                 :    0.11209\n",
      "Mean entropy                            :    4.79368\n",
      "Variance explained by the value function:    0.06004\n",
      "Std. of action_0 over agents            :    3.11415\n",
      "Std. of action_0 over envs              :    3.12983\n",
      "Std. of action_0 over time              :    3.13002\n",
      "Std. of action_1 over agents            :    3.16094\n",
      "Std. of action_1 over envs              :    3.17652\n",
      "Std. of action_1 over time              :    3.17639\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.00714\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.09900\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.42962\n",
      "Policy loss                             :   -0.19471\n",
      "Value function loss                     :    0.36039\n",
      "Mean rewards                            :    0.01830\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.52547\n",
      "Mean advantages                         :   -0.04043\n",
      "Mean (norm.) advantages                 :   -0.04043\n",
      "Mean (discounted) returns               :    0.48504\n",
      "Mean normalized returns                 :    0.48504\n",
      "Mean entropy                            :    4.77026\n",
      "Variance explained by the value function:    0.03118\n",
      "Std. of action_0 over agents            :    3.04824\n",
      "Std. of action_0 over envs              :    3.17089\n",
      "Std. of action_0 over time              :    3.17084\n",
      "Std. of action_1 over agents            :    2.93249\n",
      "Std. of action_1 over envs              :    3.05543\n",
      "Std. of action_1 over time              :    3.05457\n",
      "Current timestep                        : 210000.00000\n",
      "Gradient norm                           :    0.02195\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.36000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 31 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     169.59\n",
      "Mean action sample time per iter (ms)   :      19.62\n",
      "Mean env. step time per iter (ms)       :      49.03\n",
      "Mean training time per iter (ms)        :      45.40\n",
      "Mean total time per iter (ms)           :     294.25\n",
      "Mean steps per sec (policy eval)        :   58964.75\n",
      "Mean steps per sec (action sample)      :  509720.01\n",
      "Mean steps per sec (env. step)          :  203939.81\n",
      "Mean steps per sec (training time)      :  220274.58\n",
      "Mean steps per sec (total)              :   33984.26\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.30266\n",
      "Policy loss                             :   -0.06474\n",
      "Value function loss                     :    0.17890\n",
      "Mean rewards                            :    0.00046\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.11597\n",
      "Mean advantages                         :   -0.01346\n",
      "Mean (norm.) advantages                 :   -0.01346\n",
      "Mean (discounted) returns               :    0.10251\n",
      "Mean normalized returns                 :    0.10251\n",
      "Mean entropy                            :    4.79420\n",
      "Variance explained by the value function:    0.11795\n",
      "Std. of action_0 over agents            :    3.13920\n",
      "Std. of action_0 over envs              :    3.15509\n",
      "Std. of action_0 over time              :    3.15403\n",
      "Std. of action_1 over agents            :    3.16318\n",
      "Std. of action_1 over envs              :    3.17848\n",
      "Std. of action_1 over time              :    3.17976\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.00626\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.11800\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.11983\n",
      "Policy loss                             :    0.11524\n",
      "Value function loss                     :    0.34792\n",
      "Mean rewards                            :    0.01888\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.46941\n",
      "Mean advantages                         :    0.02441\n",
      "Mean (norm.) advantages                 :    0.02441\n",
      "Mean (discounted) returns               :    0.49382\n",
      "Mean normalized returns                 :    0.49382\n",
      "Mean entropy                            :    4.77105\n",
      "Variance explained by the value function:    0.06635\n",
      "Std. of action_0 over agents            :    3.04865\n",
      "Std. of action_0 over envs              :    3.17288\n",
      "Std. of action_0 over time              :    3.17080\n",
      "Std. of action_1 over agents            :    2.99243\n",
      "Std. of action_1 over envs              :    3.11565\n",
      "Std. of action_1 over time              :    3.11704\n",
      "Current timestep                        : 310000.00000\n",
      "Gradient norm                           :    0.02314\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.33700\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 41 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     167.33\n",
      "Mean action sample time per iter (ms)   :      19.96\n",
      "Mean env. step time per iter (ms)       :      49.46\n",
      "Mean training time per iter (ms)        :      45.71\n",
      "Mean total time per iter (ms)           :     293.23\n",
      "Mean steps per sec (policy eval)        :   59761.21\n",
      "Mean steps per sec (action sample)      :  501109.70\n",
      "Mean steps per sec (env. step)          :  202194.96\n",
      "Mean steps per sec (training time)      :  218774.16\n",
      "Mean steps per sec (total)              :   34103.32\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.20615\n",
      "Policy loss                             :    0.03187\n",
      "Value function loss                     :    0.15812\n",
      "Mean rewards                            :    0.00043\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.09050\n",
      "Mean advantages                         :    0.00671\n",
      "Mean (norm.) advantages                 :    0.00671\n",
      "Mean (discounted) returns               :    0.09721\n",
      "Mean normalized returns                 :    0.09721\n",
      "Mean entropy                            :    4.79219\n",
      "Variance explained by the value function:    0.23481\n",
      "Std. of action_0 over agents            :    3.13020\n",
      "Std. of action_0 over envs              :    3.14469\n",
      "Std. of action_0 over time              :    3.14380\n",
      "Std. of action_1 over agents            :    3.17545\n",
      "Std. of action_1 over envs              :    3.19142\n",
      "Std. of action_1 over time              :    3.19212\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.00638\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.66900\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.37900\n",
      "Policy loss                             :   -0.14408\n",
      "Value function loss                     :    0.36614\n",
      "Mean rewards                            :    0.01902\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.54397\n",
      "Mean advantages                         :   -0.02978\n",
      "Mean (norm.) advantages                 :   -0.02978\n",
      "Mean (discounted) returns               :    0.51419\n",
      "Mean normalized returns                 :    0.51419\n",
      "Mean entropy                            :    4.77166\n",
      "Variance explained by the value function:    0.09456\n",
      "Std. of action_0 over agents            :    3.06693\n",
      "Std. of action_0 over envs              :    3.18083\n",
      "Std. of action_0 over time              :    3.18029\n",
      "Std. of action_1 over agents            :    2.95592\n",
      "Std. of action_1 over envs              :    3.07860\n",
      "Std. of action_1 over time              :    3.07824\n",
      "Current timestep                        : 410000.00000\n",
      "Gradient norm                           :    0.02752\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.55000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 50 / 50\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     164.19\n",
      "Mean action sample time per iter (ms)   :      19.85\n",
      "Mean env. step time per iter (ms)       :      49.34\n",
      "Mean training time per iter (ms)        :      45.63\n",
      "Mean total time per iter (ms)           :     289.72\n",
      "Mean steps per sec (policy eval)        :   60904.88\n",
      "Mean steps per sec (action sample)      :  503900.16\n",
      "Mean steps per sec (env. step)          :  202686.43\n",
      "Mean steps per sec (training time)      :  219138.36\n",
      "Mean steps per sec (total)              :   34515.64\n",
      "========================================\n",
      "Metrics for policy 'runner'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.09581\n",
      "Policy loss                             :    0.14253\n",
      "Value function loss                     :    0.13123\n",
      "Mean rewards                            :    0.00120\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.09685\n",
      "Mean advantages                         :    0.02980\n",
      "Mean (norm.) advantages                 :    0.02980\n",
      "Mean (discounted) returns               :    0.12665\n",
      "Mean normalized returns                 :    0.12665\n",
      "Mean entropy                            :    4.79304\n",
      "Variance explained by the value function:    0.35538\n",
      "Std. of action_0 over agents            :    3.11702\n",
      "Std. of action_0 over envs              :    3.13290\n",
      "Std. of action_0 over time              :    3.13308\n",
      "Std. of action_1 over agents            :    3.14799\n",
      "Std. of action_1 over envs              :    3.16301\n",
      "Std. of action_1 over time              :    3.16367\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.00468\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    1.73000\n",
      "========================================\n",
      "Metrics for policy 'tagger'\n",
      "========================================\n",
      "VF loss coefficient                     :    0.01000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   -0.33056\n",
      "Policy loss                             :   -0.09483\n",
      "Value function loss                     :    0.31540\n",
      "Mean rewards                            :    0.01730\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.00000\n",
      "Mean value function                     :    0.47743\n",
      "Mean advantages                         :   -0.01967\n",
      "Mean (norm.) advantages                 :   -0.01967\n",
      "Mean (discounted) returns               :    0.45776\n",
      "Mean normalized returns                 :    0.45776\n",
      "Mean entropy                            :    4.77757\n",
      "Variance explained by the value function:    0.11564\n",
      "Std. of action_0 over agents            :    3.05989\n",
      "Std. of action_0 over envs              :    3.18090\n",
      "Std. of action_0 over time              :    3.18271\n",
      "Std. of action_1 over agents            :    2.94811\n",
      "Std. of action_1 over envs              :    3.06938\n",
      "Std. of action_1 over time              :    3.06774\n",
      "Current timestep                        : 500000.00000\n",
      "Gradient norm                           :    0.02202\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    9.06778\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/continuous_tag/example/1661375510/results.json' \n",
      "[Device 0]: Saving the 'runner' torch model to the file: '/tmp/continuous_tag/example/1661375510/runner_500000.state_dict'. \n",
      "[Device 0]: Saving the 'tagger' torch model to the file: '/tmp/continuous_tag/example/1661375510/tagger_500000.state_dict'. \n",
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.1__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
