{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddf8931",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee11ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33fecd",
   "metadata": {},
   "source": [
    "# Welcome to WarpDrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5b076",
   "metadata": {},
   "source": [
    "This tutorial is a first in a series of introduction notebooks for WarpDrive, a [PyCUDA](https://documen.tician.de/pycuda/)-based framework for extremely parallelized multi-agent reinforcement learning (RL) on a single graphics processing unit (GPU).\n",
    "\n",
    "In this tutorial, we describe how we harness the GPU's ability to parallelize operations across a large number of RL agents and multiple environment replicas. \n",
    "\n",
    "In conjunction with training logic using Pytorch, we can perform extremely fast end-to-end training of multiple RL agents, all on a single GPU, in just a [few lines of code](https://github.com/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bde8c",
   "metadata": {},
   "source": [
    "## GPU basics and terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f3d54",
   "metadata": {},
   "source": [
    "Before we dive into WarpDrive, let's review some GPU basics. \n",
    "\n",
    "All programs that run on a GPU need to be triggered via a CPU. Commonly, the CPU is known as the *host* and the GPU as the *device*. [CUDA](https://developer.nvidia.com/cuda-zone) (Compute Unified Device Architecture) is an extension of C that implement code to be run on (CUDA-enabled) GPU hardware.\n",
    "\n",
    "| ![gpu_memory_model](https://github.com/salesforce/warp-drive/blob/master/tutorials/assets/gpu_memory_model.png?raw=true) |\n",
    "|:--:|\n",
    "| <b>Fig. 1 The CUDA memory model</b>|\n",
    "\n",
    "CUDA launches several `threads` in parallel. It organizes threads into a group called `thread block`. Additionally, the CUDA kernel can launch multiple thread blocks, organized into a `grid` structure. \n",
    "\n",
    "Therefore, a CUDA kernel runs a grid of blocks of threads.\n",
    "\n",
    "CUDA also provides built-in variables for accessing thread information - three key variables are\n",
    "\n",
    "1. `threadIdx.x`: contains the index of the thread within the thread block.\n",
    "2. `blockIdx.x`: the index of the thread block.\n",
    "3. `blockDim.x` contains the size of thread block (number of threads in the thread block). \n",
    "\n",
    "Each CUDA card has a maximum number of threads in a block (typically, 512, 1024, or 2048).\n",
    "\n",
    "*Note: In general, threads, blocks, and grids are multidimensional, i.e., they can also have threadIdx.y and z dimensions etc. We will not go into that here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f999fd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04782ef6",
   "metadata": {},
   "source": [
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89e4c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rl_warp_drive in /home/jupyter/.local/lib/python3.8/site-packages (1.6.7)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (5.4.1)\n",
      "Requirement already satisfied: pytest>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (6.2.4)\n",
      "Requirement already satisfied: gym>=0.18 in /home/jupyter/.local/lib/python3.8/site-packages (from rl_warp_drive) (0.25.2)\n",
      "Requirement already satisfied: torch<1.11,>=1.9 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (1.10.0a0+3fd9dcf)\n",
      "Requirement already satisfied: pycuda==2021.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (2021.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.8/site-packages (from rl_warp_drive) (1.20.3)\n",
      "Requirement already satisfied: pytools>=2011.2 in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (2022.1.12)\n",
      "Requirement already satisfied: mako in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (1.2.1)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from pycuda==2021.1->rl_warp_drive) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (4.12.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gym>=0.18->rl_warp_drive) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyter/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.18->rl_warp_drive) (3.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl_warp_drive) (8.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.2.1->rl_warp_drive) (1.16.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (1.10.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (0.13.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (21.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (1.1.1)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from pytest>=6.1.0->rl_warp_drive) (0.10.2)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytools>=2011.2->pycuda==2021.1->rl_warp_drive) (2.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/lib/python3.8/site-packages (from pytools>=2011.2->pycuda==2021.1->rl_warp_drive) (4.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.8/site-packages (from mako->pycuda==2021.1->rl_warp_drive) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c94ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1f6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.managers.data_manager import CUDADataManager\n",
    "from warp_drive.managers.function_manager import CUDAFunctionManager\n",
    "from warp_drive.utils.data_feed import DataFeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb2c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c3897",
   "metadata": {},
   "source": [
    "# WarpDrive Design Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9076bb",
   "metadata": {},
   "source": [
    "Modern RL architectures ([SEED RL](https://arxiv.org/pdf/1910.06591.pdf), [ACME](https://arxiv.org/pdf/2006.00979.pdf), [IMPALA](https://arxiv.org/pdf/1802.01561.pdf), [ELF](https://arxiv.org/pdf/1707.01067.pdf), [MAVA](https://arxiv.org/pdf/2107.01460.pdf)) comprise several rollout CPU actors and a CPU/GPU trainer actor operating in tandem. While these architectures are very scalable, they may suffer from expensive communication between the CPU / GPU actors, which can lead to inefficient resource utilization. \n",
    "\n",
    "By moving both rollout generation and training exclusively to the GPU, we minimize the communication cost. In that case, all data is in the GPU's memory, and only (optional) training inspection requires a data transfer from the host to the device. We also minimize latency by having the rollout generation, batching, training and action inference all occur on the same device.\n",
    "\n",
    "Running end-to-end on a GPU is even more scalable for multi-agent RL. In essence, we can parallelize *rollouts* by having each agent operate individually on a separate thread, and each environment operate individually on a separate thread block. This results in extremely high training throughput. \n",
    "\n",
    "The figure below depicts our architecture block diagram; we will introduce `DataManager` and `FunctionManager` shortly.\n",
    "\n",
    "| ![](https://github.com/salesforce/warp-drive/blob/master/tutorials/assets/warpdrive_framework_overview.png?raw=true) |\n",
    "|:--:|\n",
    "| <b>Fig. 2 End-to-end multi-agent RL on a single GPU. Each GPU thread handles an agent, and each GPU block handles an environment. WarpDrive's DataManager and FunctionManager help manage the communication between the CPU and GPU and invoke the GPU kernel calls from the CPU, respectively.</b>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8314898",
   "metadata": {},
   "source": [
    "## PyCUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4de43d",
   "metadata": {},
   "source": [
    "Because most modern day programming is performed with Python, we have developed WarpDrive using [PyCUDA](https://documen.tician.de/pycuda/), a Python programming environment for CUDA.\n",
    "PyCUDA essentially provides additional wrappers on CUDA for easy Python access to CUDA APIs.\n",
    "\n",
    "To execute any PyCUDA program, there are three main steps:\n",
    "\n",
    "1. Copy the input data from host memory to device memory, also called *host-to-device transfer*.\n",
    "2. Load CUDA functions and execute, *caching data on-chip* for performance.\n",
    "3. Copy the results data from device memory to host memory, also called *device-to-host transfer*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283c0b3",
   "metadata": {},
   "source": [
    "## Data and Function Managers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c54df",
   "metadata": {},
   "source": [
    "Following the three steps above, we developed WarpDrive with the following two modules\n",
    "\n",
    "1. a **data manager** to handle all the data transfers between the host and the device. It also handles creating and managing the data for multiple environment replicas and time steps.\n",
    "2. a **function manager** to load the CUDA programs and execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1188a25",
   "metadata": {},
   "source": [
    "In the following, we will demonstrate how to push and pull data between the host and the device, and how to write simple CUDA functions to manipulate the date. Let's begin by creating a CUDADataManager object.\n",
    "\n",
    "We specify a few multi-agent RL parameters in the `DataManager` creator. \n",
    "\n",
    "We'll create a multi-agent RL environment with 3 agents, an episode length of 5, and 2 environment replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad0c9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(6,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(2,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(2,)\n"
     ]
    }
   ],
   "source": [
    "num_agents = 3\n",
    "num_envs = 2\n",
    "episode_length = 5\n",
    "\n",
    "cuda_data_manager = CUDADataManager(num_agents, num_envs, episode_length=episode_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7095a4",
   "metadata": {},
   "source": [
    "Now, let's create some (random) data that we would like to push to the device. In the context of RL, this can pertain to the starting states created by `env reset()`. \n",
    "\n",
    "The starting states are arrays that need to hold data such as observations, actions and rewards during the course of the episode. They could also contain environment configuration settings and hyperparameters. \n",
    "\n",
    "Each environment and agent will have its own data, so we create a `(num_envs, num_agents)`-shaped array that will be pushed to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ddb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.rand(num_envs, num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ada02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13094067, 0.33198505, 0.30681278],\n",
       "       [0.31547715, 0.40637709, 0.55222978]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9f913",
   "metadata": {},
   "source": [
    "# Push and pull data from host (CPU) to device (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd80a59",
   "metadata": {},
   "source": [
    "In order to push data to the device, we have created a **DataFeed** helper object. For all data pushed from the host to device, we will need to provide a name identifier, the actual data, and two flags (both default to False):\n",
    "\n",
    "- `save_copy_and_apply_at_reset` - if `True`, we make a copy of the starting data so that we can set the data array to that value at every environment reset, and\n",
    "- `log_data_across_episode` - if `True`, we add a time dimension to the data, of size `episode_length`, set all $t>0$ index values to zeros, and store the data array at each time step separately. This is primarily used for logging the data for an episode rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611b342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"random_data\",\n",
    "    data=random_data,\n",
    "    save_copy_and_apply_at_reset=False,\n",
    "    log_data_across_episode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed80e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_data': {'data': array([[0.13094067, 0.33198505, 0.30681278],\n",
       "         [0.31547715, 0.40637709, 0.55222978]]),\n",
       "  'attributes': {'save_copy_and_apply_at_reset': False,\n",
       "   'log_data_across_episode': False}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f314077",
   "metadata": {},
   "source": [
    "The CUDA data manager provides the **push_data_to_device()** and **pull_data_from_device()** apis to handle data transfer between the host and the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6972266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(2, 3)\n"
     ]
    }
   ],
   "source": [
    "cuda_data_manager.push_data_to_device(data_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5dfccc",
   "metadata": {},
   "source": [
    "Notice that the data manager casted the data from float64 to float32. CUDA always uses 32-bit floating or integer representations of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706f777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fetched_from_device = cuda_data_manager.pull_data_from_device(\"random_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f18e7",
   "metadata": {},
   "source": [
    "The data fetched from the device matches the data pushed (the small differences are due to type-casting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbcb2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13094066, 0.33198506, 0.30681276],\n",
       "       [0.31547713, 0.4063771 , 0.55222976]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fetched_from_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f4e74",
   "metadata": {},
   "source": [
    "Another integral part of RL is training. We also need to hold the observations, actions and rewards arrays. So fo training, we will wrap the data into a Pytorch Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e48ad2",
   "metadata": {},
   "source": [
    "## Making Training Data Accessible To PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a0c74",
   "metadata": {},
   "source": [
    "Note that pushing and pulling data several times between the host and the device causes a lot of communication overhead. So, it's advisable that we push the data from the host to device only once, and then manipulate all the data on the GPU in-place. This is particularly important when data needs to be accessed frequently. A common example is the batch of observations and rewards gathered for each training iteration. \n",
    "\n",
    "Fortunately, our framework lets Pytorch access the data we pushed onto the GPU via pointers with minimal overhead. To make data accessible by Pytorch, we set the `torch_accessible` flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3edfe693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_tensor' from type float64 to float32\n",
      "INFO:root:- random_tensor                                                                   : dtype=float32   , shape=(2, 3)\n"
     ]
    }
   ],
   "source": [
    "tensor_feed = DataFeed()\n",
    "tensor_feed.add_data(name=\"random_tensor\", data=random_data)\n",
    "\n",
    "cuda_data_manager.push_data_to_device(tensor_feed, torch_accessible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211be41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_on_device = cuda_data_manager.data_on_device_via_torch(\"random_tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9394b",
   "metadata": {},
   "source": [
    "## Time comparison for data pull (`torch_accessible` True versus False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6964e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_array = np.random.rand(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32e22c",
   "metadata": {},
   "source": [
    "### `torch_accessible=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "683a315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'large_array' from type float64 to float32\n",
      "INFO:root:- large_array                                                                     : dtype=float32   , shape=(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"large_array\",\n",
    "    data=large_array,\n",
    ")\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14cc198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.094979539000633"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Timer(lambda: cuda_data_manager.pull_data_from_device(\"large_array\")).timeit(\n",
    "    number=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f023",
   "metadata": {},
   "source": [
    "### `torch_accessible=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "431e9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'large_array_torch' from type float64 to float32\n",
      "INFO:root:- large_array_torch                                                               : dtype=float32   , shape=(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"large_array_torch\",\n",
    "    data=large_array,\n",
    ")\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9db0fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00027837299967359286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Timer(lambda: cuda_data_manager.data_on_device_via_torch(\"random_tensor\")).timeit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde9913",
   "metadata": {},
   "source": [
    "You can see the time for accessing torch tensors on the GPU is negligible compared to data arrays!\n",
    "\n",
    "Currently, the `DataManager` supports primitive data types, such as ints, floats, lists, and arrays. If you would like to push more sophisticated data structures or types to the GPU, such as dictionaries, you may do so by pushing / pulling each key-value pair as a separate array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee613eab",
   "metadata": {},
   "source": [
    "# Code Execution Inside CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00621b",
   "metadata": {},
   "source": [
    "Once we push all the relevant data to the GPU, we will need to write functions to manipulate the data. To this end, we will need to write code in CUDA C, but invoke it from the host node. The `FunctionManager` is built to facilitate function initialization on the host and execution on the device. As we mentioned before, all the arrays on GPU will be modified on the GPU, and in-place. Let's begin by creating a CUDAFunctionManager object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74d28de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_function_manager = CUDAFunctionManager(\n",
    "    num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "    num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359de96a",
   "metadata": {},
   "source": [
    "## Array manipulation inside CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da553b25",
   "metadata": {},
   "source": [
    "The benefit of GPU processing comes from the fact that we can parallelize operations across threads and grids. In the context of multi-agent RL, it makes very good sense to associate each replica of the environment to a unique block and each agent to a unique thread in the block. Accordingly, we can use the built-in CUDA variables:\n",
    "\n",
    "`int env_id = blockIdx.x;`\\\n",
    "`int agent_id = threadIdx.x;`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bda85",
   "metadata": {},
   "source": [
    "## Array indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c8371",
   "metadata": {},
   "source": [
    "An important point to understand is how multi-dimensional arrays are indexed inside CUDA. Remember that CUDA stores arrays in a C-contiguous ([row-major](https://en.wikipedia.org/wiki/Row-_and_column-major_order)) fashion (and so does Pytorch with its tensors). Accordingly, for the element at location $[i,j]$ in a data array of shape $(L,M)$, the corresponding index on CUDA is $i*M + j$.\n",
    "\n",
    "In general, for a $d$-dimensional array of shape $(N_1, N_2, \\ldots, N_d)$, the memory-offset for the element at index $(n_1, n_2, \\ldots, n_d)$ is\n",
    "$$n_d + N_d \\cdot (n_{d-1} + N_{d-1} \\cdot (n_{d-2} + N_{d-2} \\cdot (\\cdots + N_2 n_1)\\cdots)))\n",
    "= \\sum_{k=1}^d \\left( \\prod_{\\ell=k+1}^d N_\\ell \\right) n_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904e6c6",
   "metadata": {},
   "source": [
    "### Array indexing utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7bc8b",
   "metadata": {},
   "source": [
    "WarpDrive provides a convenient multi-dimensional array indexing utility function `get_flattened_array_index()` so that the above array indexing in your CUDA C code can be automatically performed\n",
    "\n",
    "`__device__ int get_flattened_array_index(const int* index_arr, const int* dim_arr, const int dimensionality);`\n",
    "\n",
    "\n",
    "The following example will get the CUDA C index for `[kEnvId, kThisAgentId]` element from an array of shape (gridDim.x, num_agents):\n",
    "\n",
    "`int global_state_arr_shape[] = {gridDim.x, num_agents};`\\\n",
    "`int agent_index[] = {kEnvId, kThisAgentId};`\\\n",
    "`int dimension = 2;`\\\n",
    "`int state_index = get_flattened_array_index(agent_index, global_state_arr_shape, dimension);`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58904c3",
   "metadata": {},
   "source": [
    "Now, let's write a simple function to add one to each element of the pushed data. We will perform this operation in parallel on the (num_envs) number of GPU blocks and the (num_agents) number of threads within.\n",
    "\n",
    "In general, the operation is (almost) parallel. Going into a bit more detail - CUDA employs a Single Instruction Multiple Thread (SIMT) architecture to manage and execute threads in groups of 32 called warps. So, as long as the number of agents is a multiple of 32, all the threads ar utilized, otherwise few threads remain idle. For example, if we use $1000$ agents, $24$ threads will remain idle, for a utilization rate of $97.65\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924140b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = \"\"\"\n",
    "// A function to demonstrate how to manipulate data on the GPU.\n",
    "// This function increments each the random data array we pushed to the GPU before.\n",
    "// Each index corresponding to (env_id, agent_id) in the array is incremented by \"agent_id + env_id\".\n",
    "// Everything inside the if() loop runs in parallel for each agent and environment.\n",
    "//\n",
    "extern \"C\"{\n",
    "    __global__ void cuda_increment(                               \n",
    "            float* data,                                  \n",
    "            int num_agents                                       \n",
    "    )                                                            \n",
    "    {                                                            \n",
    "        int env_id = blockIdx.x;                                 \n",
    "        int agent_id = threadIdx.x;                             \n",
    "        if (agent_id < num_agents){                              \n",
    "            int array_index = env_id * num_agents + agent_id;\n",
    "            int increment = env_id + agent_id;\n",
    "            data[array_index] += increment;\n",
    "        }                                                            \n",
    "    }   \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59590e",
   "metadata": {},
   "source": [
    "Notice that the keyword `__global__` is used on the increment function. Global functions are also called \"kernels\" - they are functions you may call from the host. There's also the keyword `__device__` for functions that cannot be called from the host, but may only be called from other device or global functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8c7e9",
   "metadata": {},
   "source": [
    "Next, we use the `FunctionManager` API method **load_cuda_from_source_code()** to build and load the CUDA code.\n",
    "\n",
    "*Note: We only use the string-type source code for the purposes of exposition. In general, it's standard practice to have several standalone source codes written out in cuda (.cu) file, pre-compile them to a single binary (.cubin), and then use the `FunctionManager`'s **load_cuda_from_binary_file()**.* \n",
    "\n",
    "*Additionally, if we compile template source code (so that `num_agents` and `num_envs` can be used as macro variables at compile time), we can use the CUDA `FunctionManager`'s **compile_and_load_cuda(template_header_file)**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afca3b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n"
     ]
    }
   ],
   "source": [
    "cuda_function_manager.load_cuda_from_source_code(\n",
    "    source_code, default_functions_included=False\n",
    ")\n",
    "cuda_function_manager.initialize_functions([\"cuda_increment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64083896",
   "metadata": {},
   "source": [
    "We will use the `FunctionManager`'s API method **get_function()** to load the CUDA kernel function and get an handle to invoke it from the host device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4992dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_function = cuda_function_manager.get_function(\"cuda_increment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd1fd2",
   "metadata": {},
   "source": [
    "Now, when invoking the `increment` function, along with the `data` and `num_agents` arguments, we also need to provide the block and grid arguments. These are also attributes of the CUDA `FunctionManager`: simply use\\\n",
    "\n",
    "- `block=cuda_function_manager.block`, and\n",
    "- `grid=cuda_function_manager.grid`\n",
    "\n",
    "Also, since we need to use the `num_agents` parameter, we also need to push it to the device. Instead of using a `DataFeed`, we may also push as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d84b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n"
     ]
    }
   ],
   "source": [
    "cuda_data_manager.push_data_to_device(\n",
    "    {\n",
    "        \"num_agents\": {\n",
    "            \"data\": num_agents,\n",
    "            \"attributes\": {\n",
    "                \"save_copy_and_apply_at_reset\": False,\n",
    "                \"log_data_across_episode\": False,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b40b84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_function(\n",
    "    cuda_data_manager.device_data(\"random_data\"),\n",
    "    cuda_data_manager.device_data(\"num_agents\"),\n",
    "    block=cuda_function_manager.block,\n",
    "    grid=cuda_function_manager.grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4252e20",
   "metadata": {},
   "source": [
    "Below is the original (random) data that we pushed to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3eeb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13094067, 0.33198505, 0.30681278],\n",
       "       [0.31547715, 0.40637709, 0.55222978]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4db67",
   "metadata": {},
   "source": [
    "and here's the incremented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e7f19dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13094066, 1.331985  , 2.3068128 ],\n",
       "       [1.3154771 , 2.406377  , 3.55223   ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_data_manager.pull_data_from_device(\"random_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0fc8c",
   "metadata": {},
   "source": [
    "As expected, this method incremented each entry at index `(env_id, agent_id)` of the original data by `(env_id + agent_id)`! The differences are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98358fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.65473812e-09,  9.99999945e-01,  1.99999999e+00],\n",
       "       [ 9.99999986e-01,  1.99999998e+00,  3.00000011e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_data_manager.pull_data_from_device(\"random_data\") - random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627ba93",
   "metadata": {},
   "source": [
    "And we can invoke the increment function again to increment one more time (also in-place on the GPU), and the differences double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7de0f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.65473812e-09,  1.99999995e+00,  3.99999999e+00],\n",
       "       [ 1.99999999e+00,  3.99999974e+00,  6.00000011e+00]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment_function(\n",
    "    cuda_data_manager.device_data(\"random_data\"),\n",
    "    cuda_data_manager.device_data(\"num_agents\"),\n",
    "    block=cuda_function_manager.block,\n",
    "    grid=cuda_function_manager.grid,\n",
    ")\n",
    "cuda_data_manager.pull_data_from_device(\"random_data\") - random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa5be6",
   "metadata": {},
   "source": [
    "# Validating CUDA parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf97ee",
   "metadata": {},
   "source": [
    "We put all the pieces introduced so far together, and record the times for parallelized operations with different `num_envs` and `num_agents` settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edeaa0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_random_data_and_increment_timer(\n",
    "    num_runs=1,\n",
    "    num_envs=2,\n",
    "    num_agents=3,\n",
    "    source_code=None,\n",
    "    episode_length=100,\n",
    "):\n",
    "\n",
    "    assert source_code is not None\n",
    "\n",
    "    # Initialize the CUDA data manager\n",
    "    cuda_data_manager = CUDADataManager(\n",
    "        num_agents=num_agents, num_envs=num_envs, episode_length=episode_length\n",
    "    )\n",
    "\n",
    "    # Initialize the CUDA function manager\n",
    "    cuda_function_manager = CUDAFunctionManager(\n",
    "        num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "        num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    "    )\n",
    "\n",
    "    # Load source code and initialize function\n",
    "    cuda_function_manager.load_cuda_from_source_code(\n",
    "        source_code, default_functions_included=False\n",
    "    )\n",
    "    cuda_function_manager.initialize_functions([\"cuda_increment\"])\n",
    "    increment_function = cuda_function_manager.get_function(\"cuda_increment\")\n",
    "\n",
    "    def push_random_data(num_agents, num_envs):\n",
    "        # Create random data\n",
    "        random_data = np.random.rand(num_envs, num_agents)\n",
    "\n",
    "        # Push data from host to device\n",
    "        data_feed = DataFeed()\n",
    "        data_feed.add_data(\n",
    "            name=\"random_data\",\n",
    "            data=random_data,\n",
    "        )\n",
    "        data_feed.add_data(name=\"num_agents\", data=num_agents)\n",
    "        cuda_data_manager.push_data_to_device(data_feed)\n",
    "\n",
    "    def increment_data():\n",
    "        increment_function(\n",
    "            cuda_data_manager.device_data(\"random_data\"),\n",
    "            cuda_data_manager.device_data(\"num_agents\"),\n",
    "            block=cuda_function_manager.block,\n",
    "            grid=cuda_function_manager.grid,\n",
    "        )\n",
    "\n",
    "    # One-time data push\n",
    "    data_push_time = Timer(lambda: push_random_data(num_agents, num_envs)).timeit(\n",
    "        number=1\n",
    "    )\n",
    "    # Increment the arrays 'num_runs' times\n",
    "    program_run_time = Timer(lambda: increment_data()).timeit(number=num_runs)\n",
    "\n",
    "    return {\"data push times\": data_push_time, \"code run time\": program_run_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89716292",
   "metadata": {},
   "source": [
    "## Record the times for a single data push and 10000 increment kernel calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f1a008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(1,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(1,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(1, 1)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(1,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(1,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(1, 10)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(1,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(1,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(1, 100)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(10,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(10,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(10, 10)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(1,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(1,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(1, 1000)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(100,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(100,)\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(100, 100)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(101,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(1000,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(1000,)\n",
      "WARNING:root:Warning: max number of blocks available for simultaneous run is 240, however, the number of blocks requested is 1000. Therefore, the simulation will likely under-perform\n",
      "INFO:root:Successfully build and load the source code\n",
      "INFO:root:starting to load the cuda kernel function: cuda_increment from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: cuda_increment from the CUDA module, \n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:CUDADataManager casts the data 'random_data' from type float64 to float32\n",
      "INFO:root:- random_data                                                                     : dtype=float32   , shape=(1000, 1000)\n",
      "INFO:root:- num_agents                                                                      : dtype=int32     , shape=()\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "num_runs = 10000\n",
    "times = {}\n",
    "\n",
    "for scenario in [\n",
    "    (1, 1),\n",
    "    (1, 10),\n",
    "    (1, 100),\n",
    "    (10, 10),\n",
    "    (1, 1000),\n",
    "    (100, 100),\n",
    "    (1000, 1000),\n",
    "]:\n",
    "    num_envs, num_agents = scenario\n",
    "    times.update(\n",
    "        {\n",
    "            f\"envs={num_envs}, agents={num_agents}\": push_random_data_and_increment_timer(\n",
    "                num_runs, num_envs, num_agents, source_code\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aec7364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times for 10000 function calls\n",
      "****************************************\n",
      "envs=1, agents=1              : data push time:  0.0038934s,\t mean increment times:    0.13095s\n",
      "envs=1, agents=10             : data push time:  0.0027685s,\t mean increment times:    0.13497s\n",
      "envs=1, agents=100            : data push time:  0.0028684s,\t mean increment times:    0.14054s\n",
      "envs=10, agents=10            : data push time:  0.0035858s,\t mean increment times:    0.13519s\n",
      "envs=1, agents=1000           : data push time:  0.0027829s,\t mean increment times:    0.13093s\n",
      "envs=100, agents=100          : data push time:  0.0026748s,\t mean increment times:    0.12821s\n",
      "envs=1000, agents=1000        : data push time:   0.020473s,\t mean increment times:    0.12704s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Times for {num_runs} function calls\")\n",
    "print(\"*\" * 40)\n",
    "for key, value in times.items():\n",
    "    print(\n",
    "        f\"{key:30}: data push time: {value['data push times']:10.5}s,\\t mean increment times: {value['code run time']:10.5}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8d214",
   "metadata": {},
   "source": [
    "As we increase the number of environments and agents, the data size becomes larges, so pushing data becomes slower, but since all the threads operate in parallel, the average time taken in the increment function remains about the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab942a",
   "metadata": {},
   "source": [
    "And that's it! By using building blocks such as the increment function, we can create arbitrarily complex functions in CUDA C. For some comparative examples, please see the example environments that have both Python implementations in `examples/envs` and corresponding CUDA C implementations in `src/envs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f00e86",
   "metadata": {},
   "source": [
    "Below are some useful starting resources for CUDA C programming:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0311e4a",
   "metadata": {},
   "source": [
    "- [CUDA tutorial](https://cuda-tutorial.readthedocs.io/en/latest/)\n",
    "- [Learn C](https://learnxinyminutes.com/docs/c/)\n",
    "- [CUDA Quick Reference](http://www.icl.utk.edu/~mgates3/docs/cuda.html)\n",
    "<!-- - [Thrust](https://developer.nvidia.com/thrust). Note: thrust is a flexible, high-level interface for GPU programming that greatly enhances developer productivity. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5abaa-04db-4057-8729-dd183a827123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [warpdrive] (Local)",
   "language": "python",
   "name": "local-nvcr.io_partners_salesforce_warpdrive_v1.1__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
